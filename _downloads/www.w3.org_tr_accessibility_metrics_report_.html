<!-- http://www.w3.org/TR/accessibility-metrics-report/ -->
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta http-equiv="content-type" content="text/html; charset=UTF-8" />
  <title>Research Report on Web Accessibility Metrics</title>
  <link rel="stylesheet" type="text/css" href="http://www.w3.org/StyleSheets/TR/W3C-WD" />
  <style type="text/css">
.note {
  background-color: #ffffcc;
  border: thin dashed;
  padding: 5px;
}
.highlight {
  background-color: #ffffcc;
}
.req {
  color: #000;
  padding: 5px;
  border: 1px dotted #e63232;
  background-color: #f4f3e7;
}
.termref { 
  text-decoration: none;
  color:#000000;
  padding-bottom: 0;
  margin-bottom: 0;
  border-bottom: dashed #808080 1px;
  background-color: transparent;
}
dd, li {
  margin-top: 0.25em;
}
dt {
  margin-top: 0.5em;
}
.offscreen {
  position: absolute;
  left: -999px;
  width: 990px;
}
pre{
  display:block;
  border-width:thin;
  border-style:dotted;
  border-color:#000;
  margin-left:5%;
  margin-right:15%;
  padding:5px;
  background-color:#FFC;
}



.add {
  background-color:#CCFFFF;
}
.subtract {
  background-color:#FFCCCC;
  text-decoration:line-through;
}
.open {
  background-color: #CCFFCC;
}

  </style>
</head>

<body>
<!--// head //-->
<div id="head" class="head">
  <p><a href="http://www.w3.org/"><img width="72" height="48" alt="W3C" src="http://www.w3.org/Icons/w3c_home" /></a></p>
  <h1><a id="title" name="title">Research Report on Web Accessibility Metrics</a></h1>
  <h2><a id="w3c-doctype" name="w3c-doctype"><acronym>W3C</acronym> Working Draft 30 August 2012</a></h2>
  <dl>
  <dt>This version:</dt>
    <dd><a href="http://www.w3.org/TR/2012/WD-accessibility-metrics-report-20120830/">http://www.w3.org/TR/2012/WD-accessibility-metrics-report-20120830/</a></dd>
  <dt>Latest version:</dt>
    <dd><a href="http://www.w3.org/TR/accessibility-metrics-report/">http://www.w3.org/TR/accessibility-metrics-report/</a></dd>
  <dt>Editors:</dt>
    <dd>Markel Vigo, University of Manchester</dd>
    <dd>Giorgio Brajnik, University of Udine</dd>
    <dd>Joshue O Connor, NCBI Centre for Inclusive Technology</dd>
  <dt>Citation:</dt>
    <dd>M. Vigo, G. Brajnik, J. O Connor, <abbr title="editors">eds.</abbr> Research Report on Web Accessibility Metrics. W3C WAI Research and Development Working Group (RDWG) Notes. (2012) Available at: http://www.w3.org/TR/accessibility-metrics-report</dd>
  </dl>
  <p>A <a href="./researchnote.bib">BibTex file</a> is provided; see also <a href="#proceedings_note"><strong>information on citing and referencing this document</strong></a>.</p>
  <p class="copyright"><a href="http://www.w3.org/Consortium/Legal/ipr-notice#Copyright">Copyright</a> &copy; 2012 <a href="http://www.w3.org/"><acronym title="World Wide Web Consortium">W3C</acronym></a><sup>&reg;</sup> (<a href="http://www.csail.mit.edu/"><acronym title="Massachusetts Institute of Technology">MIT</acronym></a>, <a href="http://www.ercim.eu/"><acronym title="European Research Consortium for Informatics and Mathematics">ERCIM</acronym></a>, <a href="http://www.keio.ac.jp/">Keio</a>), All Rights Reserved. W3C <a href="http://www.w3.org/Consortium/Legal/ipr-notice#Legal_Disclaimer">liability</a>, <a href="http://www.w3.org/Consortium/Legal/ipr-notice#W3C_Trademarks">trademark</a> and <a href="http://www.w3.org/Consortium/Legal/copyright-documents">document use</a> rules apply.</p>
</div>
<hr />

<h2><a id="abstract" name="abstract">Abstract</a></h2>
<p>Web accessibility metrics are an invaluable tool for researchers, developers, governmental agencies and end users. Accessibility metrics help indicate the accessibility level of websites, including the accessibility level of individual websites, or even large-scale surveys of the accessibility of many websites. Recently, a plethora of metrics has been released to complement the A, AA, and AAA Levels measurement used by the WAI guidelines. However, the validity and reliability of most of these metrics are unknown and those making use of them are taking the risk of using inappropriate metrics. In order to address these concerns, this note provides a framework that considers validity, reliability, sensitivity, adequacy and complexity as the main qualities that a metric should have.</p>
<p>A symposium was organized to observe how current practices are addressing such qualities. We found that metrics addressing validity issues are scarce although some efforts can be perceived as far as inter-tool reliability is concerned. This is something that the research community should be aware of, as we might be making efforts by using metrics whose validity and reliability are unknown. The research realm is perhaps not mature enough or we do not have the right methods and tools. We therefore try to shed some light on the possible paths that could be taken so that we can reach a maturity point.</p>

<div id="sotd">
  <h2><a id="status" name="status">Status of this document</a></h2>
  <p><em>This section describes the status of this document at the time of its publication. Other documents may supersede this document. A list of current <acronym>W3C</acronym> publications and the latest revision of this technical report can be found in the <a href="http://www.w3.org/TR/"><acronym>W3C</acronym> technical reports index</a> at http://www.w3.org/TR/.</em></p>
  <p>This 30 August 2012 First Public Working Draft of Research Report on Web Accessibility Metrics is intended to be published and maintained as a <acronym>W3C</acronym> Working Group Note after review and refinement. The note provides an initial consolidated view of the outcomes of the <a href="http://www.w3.org/WAI/RD/2011/metrics/">Website Accessibility Metrics Online Symposium</a> held on 5 December 2011.</p>
  <p>The <a href="http://www.w3.org/WAI/RD/">Research and Development Working Group (<acronym>RDWG</acronym>)</a> invites discussion and feedback on this draft document by research and practitioners interested in metrics for web accessibility, in particular by participants of the online symposium. Specifically, <acronym>RDWG</acronym> is looking for feedback on:</p>
<ul>
  <li>Summaries of the extended abstracts contributed to the online symposium;</li>
  <li>Discussion about the state-of-the-art and conclusions drawn in the document;</li>
  <li>Related resources that may be useful to the discussion within the document.</li>
</ul>
  <p>Please send comments on this Research Report on Web Accessibility Metrics document by <strong>30 September 2012</strong> to <a href="mailto:public-wai-rd-comments@w3.org">public-wai-rd-comments@w3.org</a> (publicly visible <a href="http://lists.w3.org/Archives/Public/public-wai-rd-comments/">mailing list archive</a>).</p>
  <p>Publication as a Working Draft does not imply endorsement by the <acronym>W3C</acronym> Membership. This is a draft document and may be updated, replaced or obsoleted by other documents at any time. It is inappropriate to cite this document as other than work in progress.</p>
  <p>This document has been produced by the <a href="http://www.w3.org/WAI/RD/">Research and Development Working Group (<acronym>RDWG</acronym></a>, as part of the <a href="http://www.w3.org/WAI/IPO/Activity">Web Accessibility Initiative (<acronym>WAI</acronym>) International Program Office</a>.</p>
  <p>This document was produced by a group operating under the <a href="http://www.w3.org/Consortium/Patent-Policy-20040205/">5 February 2004 <acronym>W3C</acronym> Patent Policy</a>. <acronym>W3C</acronym> maintains a <a rel="disclosure" href="http://www.w3.org/2004/01/pp-impl/47076/status">public list of any patent disclosures</a> made in connection with the deliverables of the group; that page also includes instructions for disclosing a patent. An individual who has actual knowledge of a patent which the individual believes contains <a href="http://www.w3.org/Consortium/Patent-Policy-20040205/#def-essential">Essential Claim(s)</a> must disclose the information in accordance with <a href="http://www.w3.org/Consortium/Patent-Policy-20040205/#sec-Disclosure">section 6 of the <acronym>W3C</acronym> Patent Policy</a>. </p>
</div><hr />

<div id="toc">
<h2><a id="contents" name="contents">Table of Contents</a></h2>

<ol>
  <li><a href="#introduction">Introduction</a>
    <ul>
      <li><a href="#definition_background">1.1 Definition and background</a></li>
      <li><a href="#benefits">1.2 The Benefits of Using Metrics</a></li>
    </ul>
  </li>
  <li><a href="#framework_quality_metrics">A Framework for Quality of Accessibility Metrics</a>
    <ul>
      <li><a href="#validity">2.1 Validity</a></li>
      <li><a href="#reliability">2.2 Reliability</a></li>
      <li><a href="#sensitivity">2.3 Sensitivity</a></li>
      <li><a href="#adequacy">2.4 Adequacy</a></li>
      <li><a href="#complexity">2.5 Complexity</a></li>
    </ul>
  </li>
  <li><a href="#current_research">Current Research</a>
    <ul>
      <li><a href="#addressing_validity">3.1 Addressing Validity and Reliability</a></li>
      <li><a href="#metrics_tool_support">3.2 Tool Support for Metrics</a></li>
      <li><a href="#addressing_large_scale">3.3 Addressing Large-Scale Measurement</a></li>
      <li><a href="#targeting_a11y_issues">3.4 Targeting Particular Accessibility Issues</a></li>
      <li><a href="#novel_approaches">3.5 Novel Measurement Approaches</a></li>
      <li><a href="#beyond_conformance">3.6 Beyond Conformance</a></li>
      <li><a href="#concluding_remarks">3.7 Concluding Remarks</a></li>
    </ul>
  </li>
  <li><a href="#research_roadmap">A Research Roadmap for Web Accessibility Metrics</a>
    <ul>
      <li><a href="#ensuring_quality">4.1 Ensuring Metric Quality</a></li>
      <li><a href="#metric_validity">4.2 Validity</a></li>
      <li><a href="#metric_reliability">4.3 Reliability</a></li>
      <li><a href="#other_qualities">4.4 Other Qualities</a>
        <ul>
          <li><a href="#metric_sensitivity">4.4.1 Sensitivity</a></li>
          <li><a href="#metric_adequacy">4.4.2 Adequacy</a></li>
          <li><a href="#metric_complexity">4.4.3 Complexity</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#metrics_benchmarking">A Corpus for Metrics Benchmarking</a>
    <ul>
      <li><a href="#credibility">5.1 Credibility issues</a></li>
      <li><a href="#user_tailored_metrics">5.2 User-tailored metrics</a></li>
      <li><a href="#dynamic_content">5.3 Dealing with dynamic content</a></li>
    </ul>
  </li>
  <li><a href="#conclusions">Conclusions</a></li>
  <li><a href="#references">References</a></li>
  <li><a href="#proceedings">Symposium Proceedings</a></li>
  <li><a href="#acks">Acknowledgements</a></li>
</ol>

</div>

<!--start of document contents-->
  <h2><a name="introduction" id="introduction">1. Introduction</a></h2>
  <p>The W3C/WAI Web Content Accessibility Guidelines (WCAG) and other WAI guidelines provide discrete conformance levels "A", "AA", and "AAA" to measure the level of accessibility. In many cases more granular scores would help provide a more precise indication on the level of accessibility. However, identifying valid, reliable, sensitive, adequate, and computable metrics that produce such scores is a non-trivial task with several challenges. This research report explores the qualities that such metrics need to demonstrate based on input from an <a href="http://www.w3.org/WAI/RD/2011/metrics/">Online Symposium on Website Accessibility Metrics</a> held on 5 December 2011.</p>

  <h3><a name="definition_background" id="definition_background">1.1 Definition and background</a></h3>
  <p>In the web engineering domain, a metric is a procedure for measuring a property of a web page or website. A metric can be the number of links, the size in KB of a HTML file, the number of users that click on a certain link, or the perceived ease of use of a web page. In the realm of web accessibility, amongst others, a metric can measure the following qualities:</p>
  <ul>
    <li>The number of pictures without an alt attribute.</li>
    <li>The number of Level A and AA success criteria violations.</li>
    <li>The number of possible failure points where accessibility issues can potentially happen (such as the number of images in a page).</li>
    <li>The severity of an accessibility barrier.</li>
    <li>The time taken to conduct a task.</li>
  </ul>
  <p>In order to measure more abstract qualities, more sophisticated metrics are built upon more basic ones. For instance, readability metrics [<a href="#readability">readability</a>] take into account the number of syllables, words and sentences contained in a document in order to measure the complexity of a text. Similarly, metrics aiming at measuring web accessibility have been built on specific qualities, which can be inherent in a website (such as images with no alt attribute) or observed from human behavior (e.g., user satisfaction ratings or performance indexes such as number of errors). For instance, the failure-rate metric computes the ratio between the number of accessibility violations of a particular set of criteria over the number of failure points for the same criteria.</p>
  <p>As a result of the computation of accessibility metrics, different types of data can be produced:</p>
  <ul>
    <li>Ordinal values, like WCAG 2.0 conformance levels (AAA, AA, A), or "accessible"/"non-accessible" scores; these conformance levels can be computed by a metric defined as "a web page is only accessible if all relevant success criteria are met, otherwise it is inaccessible".</li>
    <li>Quantitative ratio values such as 0, 175, -15 or 0.38.</li>
  </ul>
  <p>Web accessibility can be viewed and defined in different ways [<a href="#Brajnik08">Brajnik08</a>]. One way is to consider whether a web page/website is conformant to a set of requirements such as those defined by <a href="http://www.w3.org/WAI/intro/wcag">WCAG 2.0</a> or by <a href="http://www.access-board.gov/sec508/standards.htm">Section 508</a>. Even if WCAG 2.0 conformance levels are well specified and, as seen above, they are ordinal values, some other metrics could be defined on the basis of success criteria and their sufficient, advisory and failure techniques. We call these metrics, which are based on whether success criteria of given guidelines are met, <em>conformance-based metrics</em>.</p>
  <p>Other metrics can be defined if one assumes that accessibility is a quality that differs from conformance. For example, the US federal procurement policy known as Section 508 defines accessibility as the extent to which "a technology [...] can be used as effectively by people with disabilities as by those without it". Provided that effectiveness can be measured, such metrics could yield results that differ from conformance-based ones. Analogous to the notion of "quality in use" for software, we call these <em>accessibility-in-use</em> metrics to emphasize that they try to measure performance indexes that can be shown by real users when using the website in specific situations. In addition, they do not require the notion of conformance with respect to a set of principles. Traditional usability metrics such as effectiveness, efficiency and satisfaction could be considered accessibility-in-use metrics. Also, any measure of the perceived accessibility of a web page by users is a metric belonging to this second group. Notice that this notion of accessibility covers not only accessibility of the content of web pages, but also accessibility of user agents, features of assistive technologies, and could even address different levels of expertise that users have with these resources.</p>
  <p>Most of the existing metrics - see a review in [<a href="#Vigo11a">Vigo11a</a>] - are of the former type because they are mainly built upon criteria implemented by automated testing tools such as the number of violations or their WCAG priority. Moreover, in order to overcome the lack of sensitivity and precision of ordinal metrics, conformance metrics often yield ratio scores. The main reason for the widespread use of these types of metrics is their their low cost in terms of time and human resources since they are based on automated tools. Although no human intervention (experts' audits or user tests) is required in the process, this does not necessarily mean that only fully automated success criteria are to be considered. Some metrics estimate the violation rate of semi-automated success criteria and purely manual ones like in [<a href="#Vigo07">Vigo07</a>]; some others adopt an optimistic vs. conservative approach on their violation rate [<a href="#Lopes">Lopes</a>].</p>
  <p>The error-rate of these estimations, due to their reliance on automated testing tools are the major weaknesses of automated conformance metrics. In fact, these metrics inherit tool shortcomings such as false positives and false negatives, that affect their outcome [<a href="#Brajnik04">Brajnik04</a>].</p>
  <p>A benchmarking survey on automated conformance metrics concluded that existing metrics are quite divergent and most of them do not do a good job in distinguishing accessible pages from non-accessible pages [<a href="#Vigo11a">Vigo11a</a>]. On the other hand, there are metrics that combine testing tool metrics and those produced by human review, with the goal of estimating such errors; one example is SAMBA [<a href="#Brajnik07">Brajnik07</a>]. Other metrics do not rely on tools at all; an example is the evaluation done with the AIR method [<a href="#AIR">AIR</a>] .</p>

  <h3><a name="benefits" id="benefits">1.2 The Benefits of Using Metrics</a></h3>
  <p>There are several scenarios that could benefit from web accessibility metrics:</p>
  <ul>
    <li>Quality assurance within web engineering can exploit metrics as a way for developers to better understand the accessibility level of their artifacts throughout the development cycle.</li>
    <li>Benchmarking can exploit metrics as a way to explore, at a high-scale, the accessibility level of web pages, such as within a domain (like .gov) or within geographical areas (like in different European states).</li>
    <li>Information retrieval systems can implement metrics as one of the criteria to rank web pages. Therefore users would be able to retrieve not only pages that suit their information needs but also those that are accessible.</li>
    <li>Adaptive hypermedia techniques (e.g. adaptive navigation support) can make us of metrics  to provide guidance or as a criteria to perform interface adaptations.</li>
  </ul>

  <h2><a name="framework_quality_metrics" id="framework_quality_metrics">2. A Framework for Quality of Accessibility Metrics</a></h2>
  <p>Several quality factors can be defined for web accessibility metrics, factors that can be used to assess how applicable a metric is in a certain scenario and potentially, how to characterize the risks inherent in the use of a given metric. As discussed in [<a href="#Vigo11a">Vigo11a</a>], validity, reliability, sensitivity, adequacy and complexity appear to be the most important factors.</p>

  <h3><a name="validity" id="validity">2.1 Validity</a></h3>
  <p>This attribute is related to the extent to which the measurements obtained by a metric reflect the accessibility of the website to which it is applied, and this could depend on the notion of accessibility: conformance vs. <em>accessibility-in-use</em>. The former refers to how a web document meets specific criteria (i.e., principles and guidelines), whereas the latter indicates how the interaction is perceived. These two perspectives are not necessarily the same which can be illustrated as follows: a picture without alternative text violates a guideline making a web page non-conformant; however, the lack of alternative text may not be perceived as an obstacle if the goal of the user is to navigate or even purchase an item in a e-commerce website.</p>
  <p>As discussed above, most existing conformance metrics are plagued by their reliance on automated testing tools and do not provide means to estimate the error rate of tools. Furthermore, the way the metric itself is defined could lead to other sources of errors, reducing its validity. For example, the failure rate should not be used as a measure of accessibility-in-use; using it as a measure of conformance is also controversial: it is sometimes claimed that it measures how well developers coped with accessibility features rather than providing an estimation of conformance [<a href="#Brajnik11">Brajnik11</a>].  Validity with respect to accessibility-in-use should cope with the evaluator effect [<a href= "#Hornbaek">Hornb&aelig;k</a>] and lack of validity of users in their severity ratings [<a href="#Petrie">Petrie</a>].</p>

  <p>Validity is by far the most important quality attribute for accessibility metrics. Without it we would not know what a metric really measures. The risk of not being able to characterize validity of metrics is that potential users of metrics would choose those that appear easy to employ and that provide seemingly plausible results. In a sense, people may therefore choose a metric because it is simple rather than because it is a good metric, with the unforeseen consequence that incorrect claims and decisions could be made regarding web pages and websites. These are important issues as they strike at the heart of our notions of conformance. We are assessing the validity of a user interface without knowing if our method of assessment is actually valid itself.</p>

  <h3><a name="reliability" id="reliability">2.2 Reliability</a></h3>
  <p>This attribute is related to the reproducibility and consistency of scores, i.e. the extent to which they are the same when evaluations of the same web pages are carried out in different contexts (different tools, different people, different goals, different time). Reliability of a metric depends on several layers that are interconnected. These range from the underlying tools (what happens if we switch tools?), to underlying guidelines (what happens if we switch guidelines?), to the evaluation process itself (if random choices are made, for example when scanning a large website).</p>
  <p>The inherent inconsistency of unreliable metrics limits the ability of people to predict metric behavior; also, metrics limit the ability to be comprehended at a deeper level. However, reliability will not always be necessary. For instance, if we switch guideline sets we should not expect similar results as a different problem coverage is assumed.</p>
  <p>It is worth noting that one of the aims of this research report is to help identify errors, or spot gaps in current metrics. The idea is that we can thereby confidently reject faulty metrics, or improve them in order to halt a process of "devaluation". This devaluation happens in the mind of the end user, in terms of the perceived value of the "ideal" of conformance. This process can be a by-product of poor metrics themselves or come from misunderstanding the output from metrics that are not clear or easy for end users to understand. In other words, if a metric is not stable, it is very difficult to effectively use it as a tool of either analysis or comprehension.</p>

  <h3><a name="sensitivity" id="sensitivity">2.3 Sensitivity</a></h3>
  <p>Metric sensitivity is a measure of how changes in metric output are reflected in actual changes to any given website. Ideally we would like metrics not to be too sensitive so that they are robust and not over-reacting to small changes in web content. This is especially important when the metric is applied to highly dynamic websites as we show later in this note.</p>

  <h3><a name="adequacy" id="adequacy">2.4 Adequacy</a></h3>
  <p>This is a general quality, encompassing several properties of accessibility metrics, for instance: the type of data used to represent scores, the precision in terms of the resolution of a scale, normalization, the span covered by actual values of the metric (distribution). These attributes determine if the metric can be suitably deployed in a given scenario. For example, to be able to compare accessibility levels of different websites (as would happen in the large scale scenario discussed above) metrics should provide normalized values as otherwise comparisons are not viable. If the distribution of values of the metric is concentrated on a small interval (such as between 0.40 and 0.60, instead of [0, 1]),  large changes in accessibility could lead to small changes in the metric; round off errors could influence the final outcomes.</p>

  <h3><a name="complexity" id="complexity">2.5 Complexity</a></h3>
  <p>Depending on the type and quantity of different data and the algorithm  that is used to compute a metric, the process can be more or less computationally demanding with respect to certain resources, such as time, processors, bandwidth, memory. Therefore the complexity of a metric reflects the computational and human resources that prevent stakeholders from embracing accessibility metrics. Some scenarios rely on the fact that metrics have to be relatively simple (such as when metrics are used for adaptations of the user interface, and must be computed on the fly). However, some metrics may require high bandwidth to crawl large websites, large storage capacity or increased computing power. For those metrics that rely on human judgment, another complexity aspect is related to the workflow process that has to be established to resolve conflicts and synthesize a single value. As a result, these metrics may not suit particular application scenarios, budgets or resources.</p>

  <h2><a name="current_research" id="current_research">3. Current Research</a></h2>
  <p>The papers that were presented at the symposium cover a broad span of issues addressing the quality factors we outlined above to different extents. However, they provide new insights and ask new questions that help shaping future research avenues (see section 4).</p>

  <h3><a name="addressing_validity" id="addressing_validity">3.1 Addressing Validity and Reliability</a></h3>
  <p>Validity in terms of conformance was tackled by Vigo et al. [<a href="#Vigo11b">Vigo11b</a>] by comparing automated accessibility scores with the ones given by a panel of experts, obtaining a strong positive correlation. Inter-tool reliability of metrics was also addressed by comparing the behavior of the WAQM metric assessing 1500 pages with two different tools (EvalAccess and LIFT). A very strong correlation was found when pages were ranked according to their scores; but to obtain the same effect with ratio scores the metric requires some ad-hoc adjustment. Finally, the authors investigated inter-guideline reliability between WCAG 1.0 and WCAG 2.0 finding again a very strong correlation between ordinal values although this effect fades out when looking at ratio data.</p>
  <p>Fernandes and Benavidez [<a href="#JFernandes">JFernandes</a>] addressed metric reliability (UWEM and web@X) by comparing two tools (eChecker and eXaminator) with a different interpretation of success criteria and coverage, assessing the accessibility of about 300 pages. An initial experiment shows there is a positive moderate correlation between those tools.</p>
  <p>Reliability of metrics very often depends on the reliability of the underlying testing tools, and it is well known that different tools produce different results on the same pages. During the webinar it was noted that this problem could lead to situations where low credibility is attributed to tools and metrics; metrics would make it even more difficult to compare different outcomes and diagnose bad behavior. In addition, stakeholders could be tempted to adopt the metrics that provides the best results on their pages, or those that can be more easily interpreted and explained, regardless of whether it is related to accessibility. However, as we mention previously, we should be cautious about when we should expect reliable behavior across tools, guidelines or domain.</p>

  <h3><a name="metrics_tool_support" id="metrics_tool_support">3.2 Tool Support for Metrics</a></h3>
  <p>The availability of metrics in terms of publicly available algorithms, APIs or tools is critical for their broad adoption. Providing such mechanisms will help facilitate a broader adoption of metrics by stakeholders - especially by those that, even if interested in using them, do not have the resources to operate and articulate them. There are some incipient proposals in this direction that implement a set of metrics: Naftali and Cl&uacute;a [<a href="#Naftali">Naftali</a>] presented a platform where failure-rate and UWEM are deployed. However this does entail that human intervention is required as the system needs the input of experts to discard false positives. There are some other tools that help to keep track of the accessibility level of websites over time [<a href="#Battistelli11a">Battistelli11a</a>]. These sort of tools tend to target the accessibility monitoring of websites within determined geographical locations, normally municipalities or regional governments. The tool support provided by Fernandes et al. [<a href="#NFernandes11a">NFernandes11a</a>], QualWeb, incorporates a feature within traditional accessibility testing tools to detect templates; the novelty of this approach is that the metric employed uses the accessibility of the template as a baseline. As a result, accessibility is measured from such starting point. If the accessibility problems of the template were repaired, these fixes would automatically spread to all the pages built upon the template. Therefore, the distance from a particular web page to the template (or baseline) can be used to estimate the effort required to fix this instance, which is very valuable for quality assurance.</p>

  <h3><a name="addressing_large_scale" id="addressing_large_scale">3.3 Addressing Large-Scale Measurement</a></h3>
  <p>Large scale evaluation and measurement is required for those websites that contain a great deal of pages or when a number of websites have to be evaluated. Managing these large volumes of data cannot be done without the help of automated tools. An example of large websites is provided by Fernandes et al. [<a href="#NFernandes11a">NFernandes11a</a>]. They present a method for template detection that aims at lessening the computing effort of evaluating large amounts of pages. This is useful for websites that are substantially built on templates such as on-line stores. In the on-line stores example, normally, the only content that changes is the item to be sold and the related information; however, the layout and internal structure stays the same. One example that contemplates the measurement of the accessibility of large number of distinct websites is depicted by Battistelli et al. [<a href="#Battistelli11a">Battistelli11a</a>] using the BIF metric; similarly, AMA is a platform that enables keeping track of a large number of websites which is used to measure how conformant the websites of specific geographical locations are. Finally, Nietzio et al. [<a href="#Nietzio">Nietzio</a>] present a metric to measure WCAG 2.0 conformance in the context of a platform to keep track of the accessibility of Norwegian municipalities.</p>

  <h3><a name="targeting_a11y_issues" id="targeting_a11y_issues">3.4 Targeting Particular Accessibility Issues</a></h3>
  <p>Battistelli et al. [<a href="#Battistelli11a">Battistelli11a</a>] present a metric to quantify the compliance of documents with respect to their DTDs. Instead of measuring this compliance as if it was a binary variable (conformant/non-conformant), compliance is measured as the distance from the current document to the ideal one. Although its relationship with accessibility is not very apparent, code compliance is one of the technical accessibility requirements according to the Italian regulation and it also impacts on those success criteria that require the correct use of standards [see <a href="http://www.w3.org/TR/WCAG20/#ensure-compat-parses">WCAG 2.0 Success Criterion 4.1.1 Parsing</a>]. Also, this approach could be followed to measure accessibility. For instance, a web page could be improved until it was accessible according to guidelines or until it provides an acceptable experience to end users. The accessibility level of the non-accessible page could be computed in terms of the effort required to build the ideal web page in terms of coding lines, mark-up tags introduced or removed, or time. Another approach that tackles a particular accessibility problem is addressed by Rello and Baeza-Yates [<a href="#Rello">Rello</a>] who address the measurement of text legibility. This is something that affects the understandability of a document, a fundamental accessibility principle [see the <a href="http://www.w3.org/TR/WCAG20/#understandable"><em>Understandable</em></a> principle]. The interesting contribution of this work is its reliance on a quantitative model of spelling errors automatically computed from a large set of pages handled by a search engine. Compliance with the DTD and legibility of a web document can be considered not only accessibility success criteria but also quality issues.</p>

  <h3><a name="novel_approaches" id="novel_approaches">3.5 Novel Measurement Approaches</a></h3>
  <p>When it comes to innovative ways of measuring, the distance from a given document to a reference model can inspire similar approaches to measure web accessibility. As suggested by [<a href= "#Battistelli11b">Battistelli11b</a>], compliance can be measured by considering the distance between a given document and an ideal (or acceptable) one. In this case this distance can be measured, for instance, in terms of missing hypertext tags or effort required to accomplish changes. Another example is illustrated by measuring the distance from a instance document to a baseline template using a metric [<a href="#NFernandes11a">NFernandes11a</a>]. Another novel way of measuring accessibility can be by using a grading scale and an arbitration process, as proposed by Fischer and Wyatt [<a href="#Fischer">Fischer</a>]: the use of a five-point Likert scale aims at going beyond a binary accessible/non-accessible scoring scale. It would be interesting to see, in the future, how the final outcome of an evaluation depends on the original scores given by individual evaluators and what level of agreement exists between evaluators before arbitration takes place.</p>
  <p>Vigo [<a href="#Vigo11c">Vigo11c</a>] proposes a method by which, depending on the context, the number of checkpoints to be met changes. Nietzio et al. [<a href="#Nietzio">Nietzio</a>] suggest a stepwise method to measure conformance to WCAG 2.0, where aspects of success criteria applicability or tool support are considered. Such a method adapts to the specific testing procedures of WCAG 2.0 success criteria (SC) by providing a set of decision rules: first, the applicability of SC is analyzed; second, if applicable, the SC is tested; third, if a common failure is not found, the implementation of the sufficient techniques is checked; and finally, tool support is checked for the techniques identified in the previous step. The metric computed as a result of this process is a failure rate that takes into account also the logic underlying necessary, sufficient and counter-example techniques for each SC.</p>

  <h3><a name="beyond_conformance" id="beyond_conformance">3.6 Beyond Conformance</a></h3>
  <p>Vigo [<a href="#Vigo11c">Vigo11c</a>] proposes a method that not only considers guidelines when measuring accessibility conformance, but also considers the specific features of the device (e.g., screen size, keyboard support) as well as the assistive technology operated by the users. Including these contextual characteristics of the interaction could lead to more faithful measurements of the experience. Finally, Sloan and Kelly [<a href="#Sloan">Sloan</a>] claim that understanding accessibility as conformance to guidelines is risky in those countries (e.g., the UK) where accessibility assessment is not limited to guidelines but also focuses on the delivered service and user experience. Therefore, they encourage moving forward and embracing accessibility in terms of user experience and thinking of conformance of the production process, rather than conformance of a product that constantly changes. This perspective is novel in that it looks beyond the current conformance paradigm and aims to tap more into the user experience, and this is something that is not necessarily defined by current methods of technical validation or document conformance.</p>

  <h3><a name="concluding_remarks" id="concluding_remarks">3.7 Concluding Remarks</a></h3>
  <p>The authors of the above papers were inquired about some aspects of web accessibility metrics. The first aspect is about the target users of metrics; the goal of this question is to ascertain whether metrics researchers have in mind application scenarios or the profile of the end user who will make decisions based on the scores provided by metrics. Our survey shows that the majority of respondents do not have in mind a specific end user of metrics, or their answers are too generic. However, three papers are focused on web accessibility benchmarking (see [<a href="#Nietzio">Nietzio</a>, <a href="#Battistelli11a">Battistelli11a</a>, <a href="#JFernandes">JFernandes</a>]) and some others could be applied in this domain. This means that this is the application scenario with broader acceptance and where the application of metrics is taking off. In the remaining scenarios (quality assurance, information retrieval and adaptive web) there are also potential applications although the intent of applying in these scenarios is not evident.</p>
  <p>Second, we wanted to know whether accessibility metrics researchers are aware of the costs and risk incurred by having incorrect values for metrics. Most users consider that validity and reliability of metrics should be guaranteed although many contemplate it as future work. There is some tendency towards employing experts in such validations although most agree that users will have the last word as far as validation is concerned. This is closely related to our last question about what is the research community's point of view on measuring accessibility beyond conformance metrics. All answers we received claimed that measuring accessibility in terms of user experience should be explored more thoroughly.</p>

  <h2><a name="research_roadmap" id="research_roadmap">4. A Research Roadmap for Web Accessibility Metrics</a></h2>
  <p>This research report aims at highlighting current efforts in investigating accessibility metrics as well as uncovering existing challenges. Research on web accessibility metrics is taking off as the benefits of using them are becoming apparent; however, their adoption is far from being widespread. In addition to their relative novelty, this may occur because (1) there are a plethora of metrics out there and frameworks for metrics comparison that show their strengths and weakness are relatively recent [<a href="#Vigo11a">Vigo11a</a>]; (2) quality frameworks require further investigation as there are unexplored areas for each of the defined qualities - these areas are uncovered in section 4.1; (3) the low validity of existing metrics, which calls for a standardized testbed to show how they perform with regard to metrics quality. Setting up a corpus of web pages for benchmarking purposes could be the first step towards this goal. It would work in the same way that the Information Retrieval community does to test the performance of their algorithms [see the <a href="http://trec.nist.gov/">Text Retrieval Conference, TREC</a> ] - see section 4.2. A side-effect of the lack of validity and reliability of metrics is their lack of credibility. This could partially be tackled by the mentioned benchmarking corpus. However the credibility problem goes beyond - see section 4.3. Finally, some other issues such as user-tailored metric and dealing with dynamic content require special attention for those who aim at conducting research on web accessibility metrics.</p>

  <h3><a name="ensuring_quality" id="ensuring_quality">4.1 Ensuring Metric Quality</a></h3>
  <p>To be more precise and focusing on investigating accessibility metric quality there are still many challenges to pursue. The way a metric satisfies validity, reliability, sensitivity, adequacy and complexity qualities remains open and can be addressed by the following questions. Even if all qualities are important, we emphasize that validity and reliability of metrics should be given priority. It does not matter how sensitive or adequate a metric is, if we cannot ensure its reliability and especially validity.</p>

  <h3><a name="metric_validity" id="metric_validity">4.2 Validity</a></h3>
  <p>Studies of "validity with respect to conformance" could focus on the following research questions:</p>
  <ul>
    <li>Does validity of the metric change when we change guidelines?</li>
    <li>Does validity change when we use a subset of the guidelines?</li>
    <li>Does validity depend on the genre of the website?</li>
    <li>Is validity dependent on the type of data being provided by the testing tool?</li>
    <li>Does validity change when we switch the tool used to collect data? And what if we use data produced by merging results of two or more tools, rather than basing the metric on the data of a single tool?</li>
    <li>Are there quick ways to estimate validity of a metric?</li>
  </ul>
  <p>The above questions could be addressed in the following way:</p>
  <ul>
    <li>By a panel of judges that would systematically evaluate all the pages using the same guidelines used by the tool(s).</li>
    <li>By artificially seeding web pages with known accessibility problems (i.e. violations of guidelines), and systematically investigate how these known problems affect the metric scores.</li>
    <li>By exploring the impact on validity of manual tests when (1) they are excluded or (2) their effect is estimated.</li>
  </ul>
  <p>Studies of "validity with respect to accessibility in use" should overcome the evaluator effect [<a href="#Hornbaek">Hornb&aelig;k</a>] and lack of agreement of users in their severity ratings [<a href="#Petrie">Petrie</a>] and could address the following questions:</p>
  <ul>
    <li>Which factors affect this type of validity?</li>
    <li>Is it possible to estimate validity of the metric from other information that can be easily gathered?</li>
    <li>Is validity with respect to accessibility in use related to validity with respect to conformance?</li>
  </ul>

  <h3><a name="metric_reliability" id="metric_reliability">4.3 Reliability</a></h3>
  <p>Some efforts to understand metric reliability could go in the following direction:</p>
  <ul>
    <li>How results produced by different tools vary when applied to the same website?</li>
    <li>Study the differences in the metric scores when metrics are fed with data produced by the same tool on the same websites but when applying different guidelines.</li>
    <li>The analysis of the effects of page sampling, a process that is necessary when dealing with large websites or highly dynamic ones.</li>
    <li>See how reliability changes when merging the data produced by two or more evaluation tools applied to the same website.</li>
    <li>The analysis of how reliability of a metric correlates with its validity.</li>
  </ul>

  <h3><a name="other_qualities" id="other_qualities">4.4 Other Qualities</a></h3>

  <h4><a name="metric_sensitivity" id="metric_sensitivity">4.4.1 Sensitivity</a></h4>
  <p>Experiments could be set up to perform sensitivity analysis: given a set of accessibility problems in a test website, they could be systematically turned on or off, and their effects on metric values could be analyzed to find out which kinds of problems had the largest effect and under which circumstances. Provided that valid and reliable metrics were used, this could tell us which accessibility barriers would have a more or less strong impact on conformance or use.</p>

  <h4><a name="metric_adequacy" id="metric_adequacy">4.4.2  Adequacy</a></h4>
  <p>Provided that a metric is valid and reliable, research directions about metric adequacy should analyze the suitability and usefulness of its values for users in different scenarios, as well as metric visualization and presentation issues.</p>

  <h4><a name="metric_complexity" id="metric_complexity">4.4.3 Complexity</a></h4>
  <p class="western c2">The most important issue about metric complexity relies on its relationship with the rest of the qualities. In this regard we can pose the following questions:</p>
  <ul>
    <li>Does complexity on a metric ensure more valid and reliable results? If so, could we pursue a compromise solution between the degree of maximum complexity in a metric and its minimum validity?</li>
    <li>Can we find proxies (e.g. number of pictures in a web page) to predict the accessibility of a web page? As a side effect we could dramatically reduce the complexity on metrics.</li>
    <li>The role that the complexity of a metric plays in its adoption and employment could also be another line to follow.</li>
  </ul>

  <h2><a name="metrics_benchmarking" id="metrics_benchmarking">5. A Corpus for Benchmarking Metrics</a></h2>
  <p>One option to have a common playground so that the research community could shed some light on these challenges would be to organize the same kind of competitions as the TREC experiments. Recently, some efforts have been directed towards this goal by the <a href="http://www.w3.org/WAI/ER/tests/">W3C</a> or in the context of the <a href="http://bentoweb.org/ts">BenToWeb</a> project. There are several issues that need to be tackled.</p>
  <ul>
    <li>How do we create test collections?</li>
    <li>How do we select our test-participants? (Important in cases where the results for given metrics depend significantly on the tester)</li>
    <li>Do we make use of existing web pages?</li>
    <li>How do we inject accessibility defects in these web pages?</li>
    <li>Which criteria do we use to rank metrics?</li>
    <li>How do we isolate the metric from the underlying testing tool?</li>
    <li>Which factors should influence metrics (e.g., defects per page for a given criterion, defects repetition due to a single defect on a server-side web page template, WCAG severity level, etc.)?</li>
    <li>How do we make these outputs accessible to "non-experts"?</li>
    <li>How can we "dove tail" the user experience with metrics used in the wild?</li>
    <li>How about comparing the results of user tests with accessibility evaluation tools? This would be very interesting in terms of websites that are already borderline or considered inaccessible.</li>
  </ul>
  <p>To start with, pages we know are accessible could be collected, and pages where we know they are not (because we injected faults in them or collected from some other repositories such as <a href="http://www.fixtheweb.net/">www.fixtheweb.net</a>), and ask participants to apply their metrics to such pages and tell us how far apart are the accessible pages from the non-accessible ones. Another option would be to use pages from initiatives such as the one promoted by the WAI, "<a href="http://www.w3.org/WAI/demos/bad/">BAD: Before and After Demonstration</a>" where for educational purposes, the process of transforming a non-accessible page into an accessible one is shown.</p>

  <h3><a name="credibility" id="credibility">5.1 Credibility issues</a></h3>
  <p>Accessibility scores are a great device to grasp the accessibility level of web pages. However, metrics can turn out to be a double-edged sword: while they enhance comprehension, they can also hide relevant information and details on the accessibility of a page. This side effect can lead end users to choose the most lenient scores among those metrics that are available. As a result, there is a risk of hindering the credibility and trust of accessibility metrics.</p>
  <p>The fact that different evaluation tools yield different results directly affects metric validity and, in particular, metric reliability. The poor reproducibility of evaluation reports and accessibility scores has a side-effect on the perception of individuals in that the web accessibility assessment process can be regarded as not very credible.</p>

  <h3><a name="user_tailored_metrics" id="user_tailored_metrics">5.2 User-tailored metrics</a></h3>
  <p>There is a challenge for the personalization of metrics as not all success criteria impact all users in the same way. While some have tried to group guidelines according to their impact in determined user groups, user needs can be so specific that the effect of a given barrier is more closely related to his/her individual abilities and cannot be inferred from the fact that a particular user is identified as having a particular disability. Individual needs may deviate considerably from groups guidelines (e.g., a motor-impaired individual having more residual physical abilities than the group guidelines foresee). There are some research actions that could be taken to improve user-tailored metrics:</p>
  <ul>
    <li>Users' interaction context could be considered in metrics, encompassing the Assistive Technology (AT) they are using, the specific browser, plug-ins and operating system platform. In this regard, capturing and encapsulating user's context data in a profile would be a priority.</li>
    <li>Quantifying the relevance of guidelines: in order to tailor evaluation and measurement to the particular needs of users, accessibility barriers or checkpoint violations should be weighted according to the impact they have on determined user group or individual.</li>
    <li>Reasoning over guidelines. This way, variables that metrics normally require (priorities, number of applied guidelines) can be easily extracted and automatically inferred from violated SC.</li>
  </ul>

  <h3><a name="dynamic_content" id="dynamic_content">5.3 Dealing with dynamic content</a></h3>
  <p>Measuring something that changes over time can give different results depending of the magnitude of such changes. Modern web pages are dynamic, changing their content over time. These changes are not always a reaction to user interaction but can also be due to some other factors such as time or location. Especially in Rich Internet Applications these updates are frequently provoked by scripting techniques that mutate web contents. Therefore, the mark-up  gives few hints to predict the behavior of a web document. Normally, the most appropriate way to assess the current instance of a dynamic web document is to retrieve and test its DOM; then its subsequent mutations should be monitored and tested. As expected, different instances of a document caused by updates show inconsistent accessibility evaluation results [<a href="#NFernandes11a">Fernandes11a</a>]. As a result, if a metric is sensitive enough, it should be able to reflect this updates.</p>
  <p>This area calls for research on the frequency of the testing, that is, should pages be tested every time they update or should it be retrieved at sampling intervals? Additionally, there are some other questions: what would be the accessibility score of a determined URL if page updates entail changes in the accessibility? Should an average of all instances be cumulated?</p>
  <p>The conformance to <a href="http://www.w3.org/TR/wai-aria/">WAI-ARIA</a> and the accessibility elements subsumed by HTML5 could also be explored by future accessibility metrics.</p>

  <h2><a name="conclusions" id="conclusions">6. Conclusions</a></h2>
  <p>This research report introduces web accessibility metrics: they have been defined and specified, the benefits of using them have been highlighted and some possible application scenarios have been described. Spurred by the growing number of different metrics that are being released, we present a framework that encompasses the qualities that a good metric should have. As a result, metrics can be benchmarked according to their validity, reliability, sensitivity, adequacy and complexity. We believe this framework can help individuals to make decisions on the adoption of existing metrics according to the qualities required from metrics. In this way, there will not be the need to reinvent the wheel and design new metrics  if available metrics already fit one's needs.</p>
  <p>A symposium was held in order to check how metrics address the above-mentioned qualities and to keep track of current efforts targeting quality issues of accessibility metrics. The webinar provided a partial, but concrete, snapshot of most of the research activity around this topic We found that tool reliability is a recurrent topic in this regard, and there is still a long way to go in the realm of methods and examples for metric validity, which are rare. The editors of this research report believe that more efforts should be directed to investigate the validity and reliability of metrics. Employing metrics whose validity and reliability is questionable is a very risky practice that should be avoided. We therefore claim that accessibility metrics should be used and designed responsibly.</p>
  <p>One way to hide the inherent complexity of metrics is to provide tools that facilitate their application in an automated or semi-automated way. This need for automation comes from the necessity of assessing large volumes of data and websites; that is why large scale analysis of accessibility calls for metrics that can easily be deployed and implemented. Some other efforts are targeting specific quality aspects of the Web such as the lexical quality or the compliance to DTDs. Finally, an emerging trend aims at measuring accessibility not only in pure compliance terms. Since contextual factors play an important role in determining the quality of user experience, accessibility measurement should be able to consider these factors by collecting and including them in the measurement process or by observing the behavior and performance of real users on real settings <em>a la</em> usability testing. This perspective can be understood as a complementary approach to current accessibility measurement practice.</p>
  <p>Based on the needs and gaps that hinder current accessibility measurement we propose a number of research avenues that can help to boost the acceptance and quality of accessibility metrics. Mostly, quality issues of metric validity and reliability need urgent action but there are also some other actions that can help to make metrics more credible and widespread. A common corpus for metrics benchmarking would be a good step in this direction as it could potentially tackle quality and credibility issues at the same time. Dynamic content and user-tailoring aspects can open new research paths that can have strong impact on the quality of assessment practices, methodologies and tools.</p>

  <h2><a name="references" id="references">7. References</a></h2>
  <ul>
    <li>[<a name="AIR" id="AIR">AIR</a>] Accessibility Internet Rally (AIR). Available at
  <a href="http://www.knowbility.org/v/air/">http://www.knowbility.org/v/air/</a></li>
    <li>[<a name="Battistelli11a" id="Battistelli11a">Battistelli11a</a>] M. Battistelli, S. Mirri, L.A. Muratori, P. Salomoni (2011) Measuring accessibility barriers on large scale sets of pages. W3C WAI RDWG Symposium on Website Accessibility Metrics, paper 2. <a href="http://www.w3.org/WAI/RD/2011/metrics/paper2/">http://www.w3.org/WAI/RD/2011/metrics/paper2/</a></li>
    <li>[<a name="Battistelli11b" id="Battistelli11b">Battistelli11b</a>] M. Battistelli, S. Mirri, L.A. Muratori, P. Salomoni (2011) A metrics to make different DTDs documents evaluations comparable. W3C WAI RDWG Symposium on Website Accessibility Metrics, paper 4. <a href="http://www.w3.org/WAI/RD/2011/metrics/paper4/">http://www.w3.org/WAI/RD/2011/metrics/paper4/</a></li>
    <li>[<a name="Brajnik04" id="Brajnik04">Brajnik04</a>] G. Brajnik (2004) Comparing accessibility evaluation tools: a method for tool effectiveness. Universal Access in the Information Society 3(3-4), 252-263, DOI: 10.1007/s10209-004-0105-y</li>
    <li>[<a name="Brajnik07" id="Brajnik07">Brajnik07</a>] G. Brajnik, R. Lomuscio (2007) SAMBA: a semi-automatic method for measuring barriers of accessibility. ASSETS 2007, 43-50, DOI: 10.1145/1296843.1296853</li>
    <li>[<a name="Brajnik08" id="Brajnik08">Brajnik08</a>] G. Brajnik (2008) Beyond Conformance: The Role of Accessibility Evaluation Methods. WISE Workshops 2008, 63-80, DOI: 10.1007/978-3-540-85200-1_9</li>
    <li>[<a name="Brajnik11" id="Brajnik11">Brajnik11</a>] G. Brajnik (2011) The troubled path of accessibility engineering: an overview of traps to avoid and hurdles to overcome. ACM SIGACCESS Accessibility and Computing Newsletter, Issue 100, June 2011.</li>
    <li>[<a name="Fischer" id="Fischer">Fischer</a>] D. Fischer, T. Wyatt (2011) The case for a WCAG-based evaluation scheme with a graded rating scale. W3C WAI RDWG Symposium on Website Accessibility Metrics, paper 7. <a href="http://www.w3.org/WAI/RD/2011/metrics/paper7/">http://www.w3.org/WAI/RD/2011/metrics/paper7/</a></li>
    <li>[<a name="Hornbaek" id="Hornbaek">Hornb&aelig;k</a>] K. Hornb&aelig;k, E. Fr&oelig;kj&aelig;r (2008) A study of the evaluator effect in usability testing. Human-Computer Interaction 23 (3), 251-277, DOI: 10.1080/07370020802278205</li>
    <li>[<a name="JFernandes" id="JFernandes">JFernandes</a>] J. Fernandes, C. Benavidez (2011) A zero in eChecker equals a 10 in eXaminator: a comparison between two metrics by their scores. W3C WAI RDWG Symposium on Website Accessibility Metrics, paper 8. <a href="http://www.w3.org/WAI/RD/2011/metrics/paper8/">http://www.w3.org/WAI/RD/2011/metrics/paper8/</a></li>
    <li>[<a name="Lopes" id="Lopes">Lopes</a>] R. Lopes, D. Gomes, L. Carri&ccedil;o. (2010) Web not for all: a large scale study of web accessibility. W4A 2010, article 10, DOI: 10.1145/1805986.1806001</li>
    <li>[<a name="Naftali" id="Naftali">Naftali</a>] M. Naftali, O. Cl&uacute;a (2011) Integration of Web Accessibility Metrics into a Semi-Automatic evaluation process. W3C WAI RDWG Symposium on Website Accessibility Metrics, paper 1. <a href="http://www.w3.org/WAI/RD/2011/metrics/paper1/">http://www.w3.org/WAI/RD/2011/metrics/paper1/</a></li>
    <li>[<a name="NFernandes11a" id="NFernandes11a">NFernandes11a</a>] N. Fernandes, R. Lopes, L. Carri&ccedil;o (2011) A Template-aware Web Accessibility metric. W3C WAI RDWG Symposium on Website Accessibility Metrics, paper 3. <a href="http://www.w3.org/WAI/RD/2011/metrics/paper3/">http://www.w3.org/WAI/RD/2011/metrics/paper3/</a></li>
    <li>[<a name="Nfernandes11b" id="Nfernandes11b">NFernandes11b</a>] N. Fernandes, R. Lopes, L. Carri&ccedil;o (2011) On web accessibility evaluation environments. Proceedings of the International Cross-Disciplinary Conference on Web Accessibility, W4A 2011, article 4. DOI: 10.1145/1969289.1969295</li>
    <li>[<a name="Nietzio" id="Nietzio">Nietzio</a>] A. Nietzio, M. Eibegger, M. Goodwin, M. Snaprud (2011) Towards a score function for WCAG 2.0 benchmarking. W3C WAI RDWG Symposium on Website Accessibility Metrics, paper 11. <a href="http://www.w3.org/WAI/RD/2011/metrics/paper11/">http://www.w3.org/WAI/RD/2011/metrics/paper11/</a></li>
    <li>[<a name="Petrie" id="Petrie">Petrie</a>] H. Petrie, O. Kheir (2007) Relationship between accessibility and usability of web sites. CHI 2007, 397-406, DOI: 10.1145/1240624.1240688</li>
    <li>[<a name="readability" id="readability">readability</a>] Readability test. <a href="http://en.wikipedia.org/wiki/Readability_test">http://en.wikipedia.org/wiki/Readability_test</a></li>
    <li>[<a name="Rello" id="Rello">Rello</a>] L. Rello, R. Baeza-Yates (2011) Lexical Quality as a Measure for Textual Web Accessibility. W3C WAI RDWG Symposium on Website Accessibility Metrics, paper 5. <a href="http://www.w3.org/WAI/RD/2011/metrics/paper5/">http://www.w3.org/WAI/RD/2011/metrics/paper5/</a></li>
    <li>[<a name="Vigo07" id="Vigo07">Vigo07</a>] M. Vigo, M. Arrue, G. Brajnik, R. Lomuscio, J. Abascal (2007) Quantitative metrics for measuring web accessibility. W4A 2007, 99-107, DOI: 10.1145/1243441.1243465</li>
    <li>[<a name="Vigo11a" id="Vigo11a">Vigo11a</a>] M. Vigo and G. Brajnik (2011) Automatic web accessibility metrics: where we are and where we can go. Interacting With Computers 23(2), 137-155, DOI: doi:10.1016/j.intcom.2011.01.001</li>
    <li>[<a name="Vigo11b" id="Vigo11b">Vigo11b</a>] M. Vigo, J. Abascal, A. Aizpurua, M. Arrue (2011) Attaining Metric Validity and Reliability with the Web Accessibility Quantitative Metric. W3C WAI RDWG Symposium on Website Accessibility Metrics, paper 6. <a href="http://www.w3.org/WAI/RD/2011/metrics/paper6/">http://www.w3.org/WAI/RD/2011/metrics/paper6/</a></li>
    <li>[<a name="Vigo11c" id="Vigo11c">Vigo11c</a>] M. Vigo (2011) Context-Tailored Web Accessibility Metrics. W3C WAI RDWG Symposium on Website Accessibility Metrics, paper 9. <a href="http://www.w3.org/WAI/RD/2011/metrics/paper9/">http://www.w3.org/WAI/RD/2011/metrics/paper9/</a></li>
    <li>[<a name="Sloan" id="Sloan">Sloan</a>] D. Sloan, B. Kelly (2011) Web Accessibility Metrics For A Post Digital World. W3C WAI RDWG
  Symposium on Website Accessibility Metrics, paper 10. <a href="http://www.w3.org/WAI/RD/2011/metrics/paper10/">http://www.w3.org/WAI/RD/2011/metrics/paper10/</a></li>
  </ul>

  <h2><a name="proceedings" id="proceedings">8. Symposium Proceedings</a></h2>

  <h3><a name="proceedings_note" id="proceedings_note">Research Report on Web Accessibility Metrics</a></h3>
  <p>This document should be cited as follows:</p>
  <pre>M. Vigo, G. Brajnik, J. O Connor, eds. Research Report on Web Accessibility Metrics. 
     W3C WAI Research and Development Working Group (RDWG) Notes. (2012)
     Available at: http://www.w3.org/TR/accessibility-metrics-report</pre>
  <p>The latest version of this document is available at:</p>
  <pre><a href="http://www.w3.org/TR/accessibility-metrics-report/">http://www.w3.org/TR/accessibility-metrics-report/</a></pre>
  <p>A permanent link to this version of the document is:</p>
  <pre><a href="http://www.w3.org/TR/2012/WD-accessibility-metrics-report-20120830/">http://www.w3.org/TR/2012/WD-accessibility-metrics-report-20120830/</a></pre>
  <p>A <a href="./researchnote.bib">BibTex file</a> is provided containing:</p>
  <pre>@incollection {accessibility-metrics-report_FPWD,
  author = {W3C WAI Research and Development Working Group (RDWG)},
  title = {Research Report on Web Accessibility Metrics},
  booktitle = {W3C WAI Symposium on Website Accessibility Metrics},
  publisher = {W3C Web Accessibility Initiative (WAI)},
  year = {2012}, month = {August},
  editor = {Markel Vigo and Giorgio Brajnik and Joshue O Connor eds.},
  series = {W3C WAI Research and Development Working Group (RDWG) Notes},
  type = {Research Report},
  edition = {First Public Working Draft},
  url = {http://www.w3.org/TR/accessibility-metrics-report},
}</pre>

  <h3><a name="proceedings_papers" id="proceedings_papers">Contributed Extended Abstract Papers</a></h3>
  <p>The links provided in this section, including those in the BibTex files, are permanent; see also the <a href="http://www.w3.org/Consortium/Persistence">W3C URI Persistence Policy</a>.</p>
  <pre>@proceedings{accessibility-metrics-proceedings,
     title = {W3C WAI Symposium on Website Accessibility Metrics},
     year = {2011},
     editor = {W3C WAI Research and Development Working Group (RDWG)},
     series = {W3C WAI Research and Development Working Group (RDWG) Symposia},
     publisher = {W3C Web Accessibility Initiative (WAI)},
     url = {http://www.w3.org/WAI/RD/2011/metrics/},
}</pre>
  <ul>
    <li>M. Naftali, O. Cla. <a href="http://www.w3.org/WAI/RD/2011/metrics/paper1/">Integration of Web Accessibility Metrics into a Semi-Automatic evaluation process</a>. <a href="http://www.w3.org/WAI/RD/2011/metrics/paper1/bibtex.bib">BibTex file</a>, <a href="http://www.w3.org/WAI/RD/2011/metrics/paper1/slides.html">Slides</a>.
<pre>
@inproceedings{naftali2011,
     author = {Maia Naftali and Osvaldo Cl\'{u}a},
     title = {Integration of Web Accessibility Metrics into a Semi-Automatic 
       evaluation process},
     booktitle = {W3C WAI Symposium on Website Accessibility Metrics},
     year = {2011},
     editor = {Markel Vigo, Giorgio Brajnik, Joshue O Connor},
     pages = {article 1},
     url = {http://www.w3.org/WAI/RD/2011/metrics/paper1/},
}</pre>
    </li>
    <li>M. Battistelli, S. Mirri, L.A. Muratori, P. Salomoni. <a href="http://www.w3.org/WAI/RD/2011/metrics/paper2/">Measuring accessibility barriers on large scale sets of pages</a>. <a href="http://www.w3.org/WAI/RD/2011/metrics/paper2/bibtex.bib">BibTex file</a>, <a href="http://www.w3.org/WAI/RD/2011/metrics/paper2/slides.ppt">Slides</a>.
<pre>
@inproceedings{battistelli2011a,
     author = {Matteo Battistelli and Silvia Mirri and Ludovico Antonio Muratori 
       and Paola Salomoni},
     title = {Measuring accessibility barriers on large scale sets of pages},
     booktitle = {W3C WAI Symposium on Website Accessibility Metrics},
     year = {2011},
     editor = {Markel Vigo, Giorgio Brajnik, Joshue O Connor},
     pages = {article 2},
     url = {http://www.w3.org/WAI/RD/2011/metrics/paper2/},
}
</pre>
    </li>
    <li>N. Fernandes, R. Lopes, L. Carrio. <a href="http://www.w3.org/WAI/RD/2011/metrics/paper3/">A Template-aware Web Accessibility metric</a>. <a href="http://www.w3.org/WAI/RD/2011/metrics/paper3/bibtex.bib">BibTex file</a>.
<pre>
@inproceedings{nfernandes2011,
     author = {N\'{a}dia Fernandes and Rui Lopes and Lu\'{i}s Carri\c{c}o},
     title = {A Template-aware Web Accessibility metric},
     booktitle = {W3C WAI Symposium on Website Accessibility Metrics},
     year = {2011},
     editor = {Markel Vigo, Giorgio Brajnik, Joshue O Connor},
     pages = {article 3},
     url = {http://www.w3.org/WAI/RD/2011/metrics/paper3/},
}
</pre>
    </li>
    <li>M. Battistelli, S. Mirri, L.A. Muratori, P. Salomoni. <a href="http://www.w3.org/WAI/RD/2011/metrics/paper4/">A metrics to make different DTDs documents evaluations comparable</a>. <a href="http://www.w3.org/WAI/RD/2011/metrics/paper4/bibtex.bib">BibTex file</a>, <a href="http://www.w3.org/WAI/RD/2011/metrics/paper4/slides.ppt">Slides</a>.
<pre>
@inproceedings{battistelli2011b,
     author = {Matteo Battistelli and Silvia Mirri and Ludovico Antonio Muratori 
       and Paola Salomoni},
     title = {A metrics to make different DTDs documents evaluations comparable},
     booktitle = {W3C WAI Symposium on Website Accessibility Metrics},
     year = {2011},
     editor = {Markel Vigo, Giorgio Brajnik, Joshue O Connor},
     pages = {article 4},
     url = {http://www.w3.org/WAI/RD/2011/metrics/paper4/},
}
</pre>
    </li>
    <li>L. Rello, R. Baeza-Yates. <a href="http://www.w3.org/WAI/RD/2011/metrics/paper5/">Lexical Quality as a Measure for Textual Web Accessibility</a>. <a href="http://www.w3.org/WAI/RD/2011/metrics/paper5/bibtex.bib">BibTex file</a>, <a href="http://www.w3.org/WAI/RD/2011/metrics/paper5/slides.html">Slides</a>.
<pre>
@inproceedings{rello2011,
     author = {Luz Rello and Ricardo Baeza-Yates},
     title = {Lexical Quality as a Measure for Textual Web Accessibility},
     booktitle = {W3C WAI Symposium on Website Accessibility Metrics},
     year = {2011},
     editor = {Markel Vigo, Giorgio Brajnik, Joshue O Connor},
     pages = {article 5},
     url = {http://www.w3.org/WAI/RD/2011/metrics/paper5/},
}
</pre>
    </li>
    <li>M. Vigo, J. Abascal, A. Aizpurua, M. Arrue. <a href="http://www.w3.org/WAI/RD/2011/metrics/paper6/">Attaining Metric Validity and Reliability with the Web Accessibility Quantitative Metric</a>. <a href="http://www.w3.org/WAI/RD/2011/metrics/paper6/bibtex.bib">BibTex file</a>, <a href="http://www.w3.org/WAI/RD/2011/metrics/paper6/slides.html">Slides</a>.
<pre>
@inproceedings{vigo2011a,
     author = {Markel Vigo and Julio Abascal and Amaia Aizpurua and Myriam Arrue},
     title = {Attaining Metric Validity and Reliability with the Web Accessibility 
       Quantitative Metric},
     booktitle = {W3C WAI Symposium on Website Accessibility Metrics},
     year = {2011},
     editor = {Markel Vigo, Giorgio Brajnik, Joshue O Connor},
     pages = {article 6},
     url = {http://www.w3.org/WAI/RD/2011/metrics/paper6/},
}
</pre>
    </li>
    <li>D. Fischer, T. Wyatt. <a href="http://www.w3.org/WAI/RD/2011/metrics/paper7/">The case for a WCAG-based evaluation scheme with a graded rating scale</a>. <a href="http://www.w3.org/WAI/RD/2011/metrics/paper7/bibtex.bib">BibTex file</a>, <a href="http://www.w3.org/WAI/RD/2011/metrics/paper7/slides.ppt">Slides</a>.
<pre>
@inproceedings{fischer2011,
     author = {Detlev Fischer and Tiffany Wyatt},
     title = {The case for a WCAG-based evaluation scheme with a 
       graded rating scale},
     booktitle = {W3C WAI Symposium on Website Accessibility Metrics},
     year = {2011},
     editor = {Markel Vigo, Giorgio Brajnik, Joshue O Connor},
     pages = {article 7},
     url = {http://www.w3.org/WAI/RD/2011/metrics/paper7/},
}
</pre>
    </li>
    <li>J. Fernandes, C. Benavidez. <a href="http://www.w3.org/WAI/RD/2011/metrics/paper8/">A  zero in eChecker equals a 10 in eXaminator: a comparison between two metrics by their scores</a>. <a href="http://www.w3.org/WAI/RD/2011/metrics/paper8/bibtex.bib">BibTex file</a>, <a href="http://www.w3.org/WAI/RD/2011/metrics/paper8/slides.ppt">Slides</a>.
<pre>
@inproceedings{jfernandes2011,
     author = {Jorge Fernandes and Carlos Benavidez},
     title = {A zero in eChecker equals a 10 in eXaminator: a comparison 
       between two metrics by their scores},
     booktitle = {W3C WAI Symposium on Website Accessibility Metrics},
     year = {2011},
     editor = {Markel Vigo, Giorgio Brajnik, Joshue O Connor},
     pages = {article 8},
     url = {http://www.w3.org/WAI/RD/2011/metrics/paper8/},
}
</pre>
    </li>
    <li>M. Vigo. <a href="http://www.w3.org/WAI/RD/2011/metrics/paper9/">Context-Tailored Web Accessibility Metrics</a>. <a href="http://www.w3.org/WAI/RD/2011/metrics/paper9/bibtex.bib">BibTex file</a>, <a href="http://www.w3.org/WAI/RD/2011/metrics/paper9/slides.html">Slides</a>.
<pre>
@inproceedings{vigo2011b,
     author = {Markel Vigo},
     title = {Context-Tailored Web Accessibility Metrics},
     booktitle = {W3C WAI Symposium on Website Accessibility Metrics},
     year = {2011},
     editor = {Markel Vigo, Giorgio Brajnik, Joshue O Connor},
     pages = {article 9},
     url = {http://www.w3.org/WAI/RD/2011/metrics/paper9/},
}
</pre>
    </li>
    <li>D. Sloan, B. Kelly. <a href="http://www.w3.org/WAI/RD/2011/metrics/paper10/">Web Accessibility Metrics For A Post Digital World</a>. <a href="http://www.w3.org/WAI/RD/2011/metrics/paper10/bibtex.bib">BibTex file</a>.
<pre>
@inproceedings{sloan2011,
     author = {David Sloan and Brian Kelly},
     title = {Web Accessibility Metrics For A Post Digital World},
     booktitle = {W3C WAI Symposium on Website Accessibility Metrics},
     year = {2011},
     editor = {Markel Vigo, Giorgio Brajnik, Joshue O Connor},
     pages = {article 10},
     url = {http://www.w3.org/WAI/RD/2011/metrics/paper10/},
}
</pre>
    </li>
    <li>A. Nietzio, M. Eibegger, M. Goodwin, M. Snaprud. <a href="http://www.w3.org/WAI/RD/2011/metrics/paper11/">Towards a score function for WCAG 2.0 benchmarking</a>. <a href="http://www.w3.org/WAI/RD/2011/metrics/paper11/bibtex.bib">BibTex file</a>, <a href="http://www.w3.org/WAI/RD/2011/metrics/paper11/slides.html">Slides</a>.
<pre>
@inproceedings{nietzio2011,
 author = {Annika Nietzio and Mandana Eibegger and Morten Goodwin and Mikael Snaprud},
 title = {Towards a score function for WCAG 2.0 benchmarking},
 booktitle = {W3C WAI Symposium on Website Accessibility Metrics},
 year = {2011},
 editor = {Markel Vigo, Giorgio Brajnik, Joshue O Connor},
 pages = {article 11},
 url = {http://www.w3.org/WAI/RD/2011/metrics/paper11/},
}
</pre>
    </li>
  </ul>

  <h2><a name="acks" id="acks">9. Acknowledgements</a></h2>
  <p>Participants of the <a href="http://www.w3.org/WAI/RD/">W3C WAI Research and Development Working Group (RDWG)</a> involved in the development of this document include: Christos Kouroupetroglou, Giorgio Brajnik, Joshue O Connor, Klaus Miesenberger, Markel Vigo, Peter Thiessen, Shadi Abou-Zahra, Shawn Henry, Simon Harper, Vivienne Conway, and Yeliz Yesilada.</p>
  <p>RDWG would also like to thank the chairs and scientific committee members as well as the paper authors of the <a href="http://www.w3.org/WAI/RD/2011/metrics/">RDWG online symposium on Website Accessibility Metrics</a>.</p>
  <p>This document was developed with support from the <a href="http://www.w3.org/WAI/ACT/">WAI-ACT Project</a>.</p>

</body>
</html>