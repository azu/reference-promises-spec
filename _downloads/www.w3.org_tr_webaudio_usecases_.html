<!-- http://www.w3.org/TR/webaudio-usecases/ -->
<!DOCTYPE html>
<html dir="ltr" lang="en">
<head>
  <meta http-equiv="content-type" content="text/html; charset=UTF-8" />

    <title>Web Audio Processing: Use Cases and Requirements</title>
    <style type="text/css" media="all">
      table {border-collapse: collapse; border: 1px solid #000; font: normal 80%/140% arial, helvetica, sans-serif; color: #555; background: #fff;}
      td, th {border: 1px dotted #bbb; padding:.5em 1em; font-size: x-small; width: 10em; }
      caption {padding: 0 0 .5em 0; text-align: left; font-weight: 500; text-align: center; color: #666; background: transparent;}
      table a {padding: 1px; text-decoration: none; font-weight: bold; background: transparent;}
      table a:link {border-bottom: 1px dashed #ddd; color: #000;}
      table a:visited {border-bottom: 1px dashed #ccc; text-decoration: line-through; color: #808080;}
      table a:hover {border-bottom: 1px dashed #bbb; color: #666;}
      thead th, tfoot th {white-space: nowrap; border: 1px solid #000; text-align: center; color: black; background: #ddd;}
      tfoot td {border: 2px solid #000;}
      tbody { height: 300px; overflow: auto; }
      tbody th {color: #060606; }
      tbody th, tbody td {vertical-align: middle; text-align: center; }



/* --- INLINES --- */
em.rfc2119 { 
    text-transform:     lowercase;
    font-variant:       small-caps;
    font-style:         normal;
    color:              #900;
}

h1 acronym, h2 acronym, h3 acronym, h4 acronym, h5 acronym, h6 acronym, a acronym,
h1 abbr, h2 abbr, h3 abbr, h4 abbr, h5 abbr, h6 abbr, a abbr {
    border: none;
}

dfn {
    font-weight:    bold;
}

a.internalDFN {
    color:  inherit;
    border-bottom:  1px solid #99c;
    text-decoration:    none;
}

a.externalDFN {
    color:  inherit;
    border-bottom:  1px dotted #ccc;
    text-decoration:    none;
}

a.bibref {
    text-decoration:    none;
}

cite .bibref {
    font-style: normal;
}

code {
    color:  #ff4500;
}


/* --- --- */
ol.algorithm { counter-reset:numsection; list-style-type: none; }
ol.algorithm li { margin: 0.5em 0; }
ol.algorithm li:before { font-weight: bold; counter-increment: numsection; content: counters(numsection, ".") ") "; }

/* --- TOC --- */
.toc a, .tof a {
    text-decoration:    none;
}

a .secno, a .figno {
    color:  #000;
}

ul.tof, ol.tof {
    list-style: none outside none;
}

.caption {
    margin-top: 0.5em;
    font-style:   italic;
}

/* --- TABLE --- */
table.simple {
    border-spacing: 0;
    border-collapse:    collapse;
    border-bottom:  3px solid #005a9c;
}

.simple th {
    background: #005a9c;
    color:  #fff;
    padding:    3px 5px;
    text-align: left;
}

.simple th[scope="row"] {
    background: inherit;
    color:  inherit;
    border-top: 1px solid #ddd;
}

.simple td {
    padding:    3px 10px;
    border-top: 1px solid #ddd;
}

.simple tr:nth-child(even) {
    background: #f0f6ff;
}

/* --- DL --- */
.section dd > p:first-child {
    margin-top: 0;
}

.section dd > p:last-child {
    margin-bottom: 0;
}

.section dd {
    margin-bottom:  1em;
}

.section dl.attrs dd, .section dl.eldef dd {
    margin-bottom:  0;
}
</style> 
 <link rel="stylesheet" type="text/css" href="http://www.w3.org/StyleSheets/TR/W3C-WG-NOTE"/>
  </head>
  <body>
    <div class="head">
      <p> <a href="http://www.w3.org/"><img alt="W3C" src="http://www.w3.org/Icons/w3c_home"
            height="48" width="72"> </a> </p>
      <h1 class="title" id="title">Web Audio Processing: Use Cases and
        Requirements</h1>
      <h2 id="w3c-WG-NOTE"><abbr title="World Wide Web Consortium">W3C</abbr>
        Working Group Note 29 January 2013</h2>
      <dl>
        <dt>This version:</dt>
        <dd><a href="http://www.w3.org/TR/2013/NOTE-webaudio-usecases-20130129/">http://www.w3.org/TR/2013/NOTE-webaudio-usecases-20130129/</a></dd>
        <dt>Latest published version:</dt>  
       <dd><a href="http://www.w3.org/TR/webaudio-usecases/">http://www.w3.org/TR/webaudio-usecases/</a></dd>
        
        <dt>Previous version:</dt>
        <dd><a href="http://www.w3.org/TR/2012/WD-webaudio-usecases-20121004/">http://www.w3.org/TR/2012/WD-webaudio-usecases-20121004/</a></dd>
        
        
        
        
        
        
        
        <dt>Latest editor's draft:</dt>
        <dd><a href="https://dvcs.w3.org/hg/audio/raw-file/tip/reqs/Overview.html">https://dvcs.w3.org/hg/audio/raw-file/tip/reqs/Overview.html</a></dd>
        <dt>Editors:</dt>
        <dd><span>Joe Berkovitz</span>, <a href="http://www.noteflight.com/">Noteflight</a></dd>
        <dd><span>Olivier Thereaux</span>, <a href="http://bbc.co.uk/">British
            Broadcasting Corporation (BBC)</a></dd>
      </dl>
      <p class="copyright"> <a href="http://www.w3.org/Consortium/Legal/ipr-notice#Copyright">Copyright</a>
        © 2013 <a href="http://www.w3.org/"><abbr title="World Wide Web Consortium">W3C</abbr></a><sup>®</sup>
        (<a href="http://www.csail.mit.edu/"><abbr title="Massachusetts Institute of Technology">MIT</abbr></a>,
        <a href="http://www.ercim.eu/"><abbr title="European Research Consortium for Informatics and Mathematics">ERCIM</abbr></a>,
        <a href="http://www.keio.ac.jp/">Keio</a>), All Rights Reserved. <abbr
          title="World Wide Web Consortium">W3C</abbr> <a href="http://www.w3.org/Consortium/Legal/ipr-notice#Legal_Disclaimer">liability</a>,
        <a href="http://www.w3.org/Consortium/Legal/ipr-notice#W3C_Trademarks">trademark</a>
        and <a href="http://www.w3.org/Consortium/Legal/copyright-documents">document
          use</a> rules apply. </p>
      <hr> </div>
    <section id="abstract" class="introductory">
      <h2>Abstract</h2>

      <p>This document introduces a series of scenarios and a list of
        requirements guiding the work of the <abbr title="World Wide Web Consortium">W3C</abbr>
        Audio Working Group in its development of a web API for processing and
        synthesis of audio on the web. </p>
    </section>
    <section id="sotd" class="introductory">
      <h2>Status of This Document</h2>
      <p> <em>This section describes the status of this document at the time of
          its publication. Other documents may supersede this document. A list
          of current <abbr title="World Wide Web Consortium">W3C</abbr>
          publications and the latest revision of this technical report can be
          found in the <a href="http://www.w3.org/TR/"><abbr title="World Wide Web Consortium">W3C</abbr>
            technical reports index</a> at http://www.w3.org/TR/.</em> </p>
    
      
      <p> This document was published by the <a href="http://www.w3.org/2011/audio/">Audio
          Working Group</a> as a Working Group Note. If you wish to make comments
        regarding this document, please send them to <a href="mailto:public-audio@w3.org">public-audio@w3.org</a>
        (<a href="mailto:public-audio-request@w3.org?subject=subscribe">subscribe</a>,
        <a href="http://lists.w3.org/Archives/Public/public-audio/">archives</a>).
        All comments are welcome. </p>
      <p>Publication as a Working Group Note does not imply endorsement by the W3C Membership. This is a draft document and may be updated, replaced or obsoleted by other documents at any time. It is inappropriate to cite this document as other than work in progress. </p>
    
      <p> This document was produced by a group operating under the <a href="http://www.w3.org/Consortium/Patent-Policy-20040205/">5
          February 2004 <abbr title="World Wide Web Consortium">W3C</abbr>
          Patent Policy</a>. <abbr title="World Wide Web Consortium">W3C</abbr>
        maintains a <a href="http://www.w3.org/2004/01/pp-impl/46884/status" rel="disclosure">public
          list of any patent disclosures</a> made in connection with the
        deliverables of the group; that page also includes instructions for
        disclosing a patent. An individual who has actual knowledge of a patent
        which the individual believes contains <a href="http://www.w3.org/Consortium/Patent-Policy-20040205/#def-essential">Essential
          Claim(s)</a> must disclose the information in accordance with <a href="http://www.w3.org/Consortium/Patent-Policy-20040205/#sec-Disclosure">section
          6 of the <abbr title="World Wide Web Consortium">W3C</abbr> Patent
          Policy</a>. </p>
    </section>
    <section id="toc">
      <h2 class="introductory">Table of Contents</h2>
      <ul class="toc">
        <li class="tocline"><a class="tocxref" href="#introduction"><span class="secno">1.
              </span>Introduction</a></li>
        <li class="tocline"><a class="tocxref" href="#web-audio-scenarios"><span

              class="secno">2. </span>Web Audio Scenarios</a>
          <ul class="toc">
            <li class="tocline"><a class="tocxref" href="#video-chat-application"><span

                  class="secno">2.1 </span>Video Chat Application</a></li>
            <li class="tocline"><a class="tocxref" href="#x3d-game-with-music-and-convincing-sound-effects"><span

                  class="secno">2.2 </span>3D game with music and convincing
                sound effects</a></li>
            <li class="tocline"><a class="tocxref" href="#online-music-production-tool"><span

                  class="secno">2.3 </span>Online music production tool</a></li>
            <li class="tocline"><a class="tocxref" href="#online-radio-broadcast"><span

                  class="secno">2.4 </span>Online radio broadcast</a></li>
            <li class="tocline"><a class="tocxref" href="#music-creation-environment-with-sampled-instruments"><span

                  class="secno">2.5 </span>Music Creation Environment with
                Sampled Instruments</a></li>
            <li class="tocline"><a class="tocxref" href="#connected-dj-booth"><span

                  class="secno">2.6 </span>Connected <abbr title="Disc Jockey">DJ</abbr>
                booth</a></li>
            <li class="tocline"><a class="tocxref" href="#playful-sonification-of-user-interfaces"><span

                  class="secno">2.7 </span>Playful sonification of user
                interfaces</a></li>
            <li class="tocline"><a class="tocxref" href="#podcast-on-a-flight"><span

                  class="secno">2.8 </span>Podcast on a flight</a></li>
            <li class="tocline"><a class="tocxref" href="#short-film-with-director-s-commentary-and-audio-description"><span

                  class="secno">2.9 </span>Short film with director's
                commentary and audio description</a></li>
            <li class="tocline"><a class="tocxref" href="#web-based-guitar-practice-service"><span

                  class="secno">2.10 </span>Web-based guitar practice service </a></li>
            <li class="tocline"><a class="tocxref" href="#user-control-of-audio"><span

                  class="secno">2.11 </span>User Control of Audio</a></li>
          </ul>
        </li>
        <li class="tocline"><a class="tocxref" href="#acknowledgements"><span class="secno">A.
              </span>Acknowledgements</a></li>
      </ul>
    </section>
    <section id="introduction">
      <!--OddPage-->
      <h2><span class="secno">1. </span>Introduction</h2>
      <p>What should the future web sound like? That was, in essence, the
        mission of the <abbr title="World Wide Web Consortium">W3C</abbr> Audio
        Working Group when it was chartered in early 2011 to "support the
        features required by advanced interactive applications including the
        ability to process and synthesize audio". Bringing audio processing and
        synthesis capabilities to the Open Web Platform should allow developers
        to re-create well-loved audio software on the open web and add great
        sound to web games and applications; it may also enable web developers
        to reinvent the world of audio and music by making it more connected,
        linked and social.</p>
      <p>This document attempts to describe the scenarios considered by the <abbr

          title="World Wide Web Consortium">W3C</abbr> Audio Working Group in
        its work to define Web Audio technologies. Not intended to be a
        comprehensive list of things which the Web Audio standards will make
        possible, it nevertheless attempts to:</p>
      <ul>
        <li>document a number of key applications of audio which Web Audio
          standards should enable,</li>
        <li>provide a basis for discussion on the desired architecture of Web
          Audio standards,</li>
        <li>offer examples for early uses of the technology, which can then be
          used to gather feedback on the draft standard, and</li>
        <li>extract technical and architectural requirements for the Web Audio
          APIs or libraries built upon it. The <em>Notes and Implementation
            Considerations</em> sections will note which constructs of the <a href="http://www.w3.org/TR/webaudio/"

            title="Web Audio API">Web Audio API</a> Working Draft apply.
          Whenever possible, the document will note which features are yet to be
          implemented or documented in that specification as of <a href="http://www.w3.org/TR/2012/WD-webaudio-20120802/"

            title="Web Audio API">02 August 2012</a>.</li>
      </ul>
    </section>
    <section id="web-audio-scenarios">
      <!--OddPage-->
      <h2><span class="secno">2. </span>Web Audio Scenarios</h2>
      <p>This section will introduce a number of scenarios involving the use of
        Web Audio processing or synthesis technologies, and discuss
        implementation and architectural considerations.</p>
      <section id="video-chat-application">
        <h3><span class="secno">2.1 </span>Video Chat Application</h3>
        <p>Three people have joined a three-way conversation through a web
          application. Each of them see the other two participants in split
          windows, and hear their voice in sync with the video.</p>
        <p>The application provides a simple interface to control the incoming
          audio and video of the other participants: at any time, the user can
          mute the incoming streams, control the overall sound volume, or mute
          themselves while continuing to send a live video stream through the
          application.</p>
        <p>Advanced controls are also available. In the "Audio" option panel,
          the user has the ability to adapt the incoming sound to their taste
          through a graphic equalizer interface, as well as a number of filters
          for voice enhancement, a feature which can be useful between people
          with hearing difficulties, in imperfect listening environments, or to
          compensate for poor transmission environments.</p>
        <p>Another option allows the user to change the spatialization of the
          voices of their interlocutors; the default is a binaural mix matching
          the disposition of split-windows on the screen, but the interface
          makes it possible to reverse the left-right balance, or make the other
          participants appear closer or further apart.</p>
        <p>The makers of the chat applications also offer a "fun" version which
          allows users to distort (pitch, speed, other effects) their voice.
          They are considering adding the option to the default software, as
          such a feature could also be used to protect one participants' privacy
          in some contexts.</p>
        <h3 id="notes-and-implementation-considerations">Notes and
          Implementation Considerations</h3>
        <ol>
          <li>
            <p>The processing capabilities needed by this scenario include:</p>
            <ul>
              <li>Mixing and spatialization of several sound sources</li>
              <li><em>Controlling the gain</em> (mute and volume control) of
                several audio sources</li>
              <li><em>Filtering</em> (<abbr title="Equalizer">EQ</abbr>, voice
                enhancement)</li>
              <li>Modifying the pitch and speed of sound sources</li>
            </ul>
          </li>
          <li>
            <p>This scenario is also a good example of the need for audio
              capture (from line in, internal microphone or other inputs). We
              expect this to be provided by <a href="http://www.w3.org/TR/html-media-capture/"

                title="HTML Media Capture">HTML Media Capture</a>.</p>
          </li>
          <li>
            <p>The <a href="http://tools.ietf.org/html/draft-ietf-rtcweb-use-cases-and-requirements-06#section-4.2.1">first
                scenario in WebRTC's Use Cases and Requirements document</a> has
              been a strong inspiration for this scenario. Most of the
              technology, described above should be covered by the <a href="http://www.w3.org/TR/webrtc/"

                title="WebRTC 1.0: Real-time Communication Between Browsers">Web
                Real-Time Communication API</a>. The scenario illustrates,
              however, the need to integrate audio processing with the handling
              of RTC streams, with a technical requirement for processing of the
              audio signal at both ends (capture of the user's voice and output
              of its correspondents' conversation).</p>
          </li>
          <li>
            <p>Speed changes are currently unsupported by the Web Audio API.</p>
          </li>
        </ol>
      </section>
      <section id="x3d-game-with-music-and-convincing-sound-effects">
        <h3><span class="secno">2.2 </span>3D game with music and convincing
          sound effects</h3>
        <p>A commuter is playing a 3D first-person adventure game on their
          mobile device. The game is built entirely using open web technologies,
          and includes rich, convincing sound piped through the commuter's
          stereo headphones.</p>
        <p>As soon as the game starts, a musical background starts, loops
          seamlessly, and transitions smoothly from one music track to another
          as the player enters a house. Some of the music is generated live, and
          reacts to the state of the game: tempo, time signature, note
          properties and envelopes change depending on the the health level of
          the characters and their actions.</p>
        <p>While walking in a corridor, the player can hear the muffled sound of
          a ticking grandfather's clock. Following the direction of the sound
          and entering a large hall, the sound of the clock becomes clear,
          reverberating in the large hall. At any time, the sound of the clock
          spatialized in real-time based on the position of the player's
          character in the room (relative to the clock) and the current camera
          angle.</p>
        <p>The soundscape changes, bringing a more somber, scary atmosphere to
          the scene: the once full orchestral underscore is slowly reduced,
          instrument by instrument, to a lonely and echoing cello. The player
          equips a firearm. Suddenly, a giant snake springs from behind a
          corner, its hissing becoming a little louder as the snake turns its
          head towards the player. The weapon fires at the touch of a key, and
          the player can hear the sound of bullets in near-perfect
          synchronization with the firing, as well as the sound of bullets
          ricocheting against walls. The sounds are played immediately after the
          player presses the key, but the action and video frame rate can remain
          smooth even when a lot of sounds (bullets being fired, echoing and
          ricocheting, sound of the impacts, etc) are played at the same time.
          The snake is now dead, and many flies gather around it, and around the
          player's character, buzzing and zooming in the virtual space of the
          room.</p>
        <h3 id="notes-and-implementation-considerations-1">Notes and
          Implementation Considerations</h3>
        <ol>
          <li>
            <p>Developing the soundscape for a game as the one described above
              can benefit from a <em>modular, node based approach</em> to audio
              processing. In our scenario, some of the processing needs to
              happen for a number of sources at the same time (e.g room effects)
              while others (e.g mixing and spatialization) need to happen on a
              per-source basis. A graph-based API makes it very easy to
              envision, construct and control the necessary processing
              architecture, in ways that would be possible with other kinds of
              APIs, but more difficult to implement. The fundamental <code>AudioNode</code>
              construct in the Web Audio API supports this approach.</p>
          </li>
          <li>
            <p>While a single looping music background can be created today with
              the <a href="http://www.w3.org/TR/html5/embedded-content-0.html#the-audio-element"

                title="4.8.7 The audio element â€” HTML5">HTML5 &lt;audio&gt;
                element</a>, the ability to transition smoothly from one musical
              background to another requires additional capabilities that are
              found in the Web Audio API including <em>sample-accurate playback
                scheduling</em> and <em>automated cross-fading of multiple
                sources</em>. Related API features include <code>AudioBufferSourceNode.start()</code>
              and <code>AudioParam.setValueAtTime()</code>.</p>
          </li>
          <li>
            <p>The musical background of the game not only involves seamless
              looping and transitioning of full tracks, but also the automated
              creation of generative music from basic building blocks or
              algorithms ("Some of the music is generated live, and reacts to
              the state of the game"), as well as the creation and evolution of
              a musical score from multiple instrument tracks ("the once full
              orchestral underscore is slowly reduced, instrument by
              instrument"). Related requirements for such features are developed
              in details within the <a href="#online-music-production-tool">Online
                music production tool</a> and <a href="#music-creation-environment-with-sampled-instruments">Music
                Creation Environment with Sampled Instruments</a> scenarios.</p>
          </li>
          <li>
            <p>The scenario illustrates many aspects of the creation of a
              credible soundscape. The game character is evolving in a virtual
              three-dimensional environment and the soundscape is at all times
              spatialized: a <em>panning model</em> can be used to spatialize
              sound sources in the game (<code>AudioPanningNode</code>); <em>obstruction
                / occlusion</em> modeling is used to muffle the sound of the
              clock going through walls, and the sound of flies buzzing around
              would need <em>Doppler Shift</em> simulation to sound believable
              (also supported by <code>AudioPanningNode</code>). The listener's
              position is part of this 3D model as well (<code>AudioListener</code>).</p>
          </li>
          <li>
            <p>As the soundscape changes from small room to large hall, the game
              benefits from the <em>simulation of acoustic spaces</em>,
              possibly through the use of a <em>convolution engine</em> for
              high quality room effects as supported by <code>ConvolverNode</code>
              in the Web Audio API.</p>
          </li>
          <li>
            <p>Many sounds in the scenario are triggered by events in the game,
              and would need to be played with low latency. The sound of the
              bullets as they are fired and ricochet against the walls, in
              particular, illustrate a requirement for <em>basic polyphony</em>
              and <em>high-performance playback and processing of many sounds</em>.
              These are supported by the general ability of the Web Audio API to
              include many sound-generating nodes with independent scheduling
              and high-throughput native algorithms.</p>
          </li>
        </ol>
      </section>
      <section id="online-music-production-tool">
        <h3><span class="secno">2.3 </span>Online music production tool</h3>
        <p>A music enthusiast creates a musical composition from audio media
          clips using a web-based Digital Audio Workstation (DAW) application.</p>
        <p>Audio "clips" are arranged on a timeline representing multiple tracks
          of audio. Each track's volume, panning, and effects may be controlled
          separately. Individual tracks may be muted or soloed to preview
          various combination of tracks at a given moment. Audio effects may be
          applied per-track as inline (insert) effects. Additionally, each track
          can send its signal to one or more global send effects which are
          shared across tracks. Sub-mixes of various combinations of tracks can
          be made, and a final mix bus controls the overall volume of the mix,
          and may have additional insert effects.</p>
        <p>Insert and send effects include dynamics compressors (including
          multi-band), extremely high-quality reverberation, filters such as
          parametric, low-shelf, high-shelf, graphic <abbr title="equalizer">EQ</abbr>,
          etc. Also included are various kinds of delay effects such as
          ping-pong delays, and <abbr title="Beats Per Minute">BPM</abbr>-synchronized
          delays with feedback. Various kinds of time-modulated effects are
          available such as chorus, phasor, resonant filter sweeps, and <abbr title="Beats Per Minute">BPM</abbr>-synchronized
          panners. Distortion effects include subtle tube simulators, and
          aggressive bit decimators. Each effect has its own <abbr title="User Interface">UI</abbr>
          for adjusting its parameters. Real-time changes to the parameters can
          be made (e.g. with a mouse) and the audible results heard with no
          perceptible lag.</p>
        <p>Audio clips may be arranged on the timeline with a high-degree of
          precision (with sample accurate playback). Certain clips may be
          repeated loops containing beat-based musical material, and are
          synchronized with other such looped clips according to a certain
          musical tempo. These, in turn, can be synchronized with sequences
          controlling real-time synthesized playback. The values of volume,
          panning, send levels, and each parameter of each effect can be changed
          over time, displayed and controlled through a powerful <abbr title="User Interface">UI</abbr>
          dealing with automation curves. These curves may be arbitrary and can
          be used, for example, to control volume fade-ins, filter sweeps, and
          may be synchronized in time with the music (beat synchronized).</p>
        <p>Visualizers may be applied for technical analysis of the signal.
          These visualizers can be as simple as displaying the signal level in a
          <abbr title="Volume Unit">VU</abbr> meter, or more complex such as
          real-time frequency analysis, or <abbr title="Left/Right">L/R</abbr>
          phase displays.</p>
        <p>The actual audio clips to be arranged on the timeline are managed in
          a library of available clips. These can be searched and sorted in a
          variety of ways and with high-efficiency. Although the clips can be
          cloud-based, local caching offers nearly instantaneous access and
          glitch-free playback.</p>
        <p>The final mix may be rendered at faster than real-time and then
          uploaded and shared with others. The session representing the clips,
          timeline, effects, automation, etc. may also be shared with others for
          shared-mixing collaboration.</p>
        <h3 id="notes-and-implementation-considerations-2">Notes and
          Implementation Considerations</h3>
        <ol>
          <li>
            <p>This scenario details the large number of feature requirements
              typically expected of professional audio software or hardware. It
              encompasses many advanced audio control capabilities such as
              filtering, effects, dynamics compression and control of various
              audio parameters.</p>
          </li>
          <li>
            <p>Building such an application may only be reasonably possible if
              the technology enables the control of audio with acceptable
              performance, in particular for <em>real-time processing</em> and
              control of audio parameters and <em>sample accurate scheduling of
                sound playback</em>. Because performance is such a key aspect of
              this scenario, it should probably be possible to control the
              buffer size of the underlying Audio API: this would allow users
              with slower machines to pick a larger buffer setting that does not
              cause clicks and pops in the audio stream.</p>
          </li>
          <li>
            <p>The ability to visualize the samples and their processing
              benefits from <em>real-time time-domain and frequency analysis</em>,
              as supplied by the Web Audio API's <code>RealtimeAnalyzerNode</code>.</p>
          </li>
          <li>
            <p> Clips must be able to be loaded into memory for fast playback.
              The Web Audio API's <code>AudioBuffer</code> and <code>AudioBufferSourceNode</code>
              interfaces address this requirement.</p>
          </li>
          <li>
            <p> Some sound sources may be purely algorithmic in nature, such as
              oscillators or noise generators. This implies the ability to
              generate sound from both precomputed and dynamically computed
              arbitrary sound samples. The Web Audio API's ability to create an
              <code>AudioBuffer</code> from arrays of numerical samples, coupled
              with the ability of <code>JavaScriptAudioNode</code> to supply
              numerical samples on the fly, both address this requirement.</p>
          </li>
          <li>
            <p>The ability to schedule both audio clip playback and effects
              parameter value changes in advance is essential to support
              automated mixdown</p>
          </li>
          <li>
            <p> To export an audio file, the audio rendering pipeline must be
              able to yield buffers of sample frames directly, rather than being
              forced to an audio device destination. Built-in codecs to
              translate these buffers to standard audio file output formats are
              also desirable.</p>
          </li>
          <li>
            <p>Typical per-channel effects such as panning, gain control,
              compression and filtering must be readily available in a native,
              high-performance implementation.</p>
          </li>
          <li>
            <p>Typical master bus effects such as room reverb must be readily
              available. Such effects are applied to the entire mix as a final
              processing stage. A single <code>ConvolverNode</code> is capable
              of simulating a wide range of room acoustics.</p>
          </li>
        </ol>
      </section>
      <section id="online-radio-broadcast">
        <h3><span class="secno">2.4 </span>Online radio broadcast</h3>
        <p>A web-based online radio application supports one-to-many audio
          broadcasting on various channels. For any one broadcast channel it
          exposes three separate user interfaces on different pages. One
          interface is used by the broadcaster controlling a radio show on the
          channel. A second interface allows invited guests to supply live audio
          to the show. The third interface is for the live online audience
          listening to the channel.</p>
        <p>The broadcaster interface supports live and recorded audio source
          selection as well as mixing of those sources. Audio sources include:</p>
        <ul>
          <li>any local microphone</li>
          <li>prerecorded audio such as jingles or tracks from music libraries</li>
          <li>a remote microphone for a remote guest</li>
        </ul>
        <p>A simple mixer lets the broadcaster control the volume, pan and
          effects processing for each local or remote audio source, blending
          them into a single stereo output mix that is broadcast as the show's
          content. Indicators track the level of each active source. This mixer
          also incorporates some automatic features to make the broadcaster's
          life easier, including ducking of prerecorded audio sources when any
          local or remote microphone source is active. Muting (un-muting) of
          sources causes an automatic fast volume fade-out(in) to avoid audio
          transients. The broadcaster can hear a live monitor mix through
          headphones, with an adjustable level for monitoring their local
          microphone.</p>
        <p>The application is aware of when prerecorded audio is playing in the
          mix, and each audio track's descriptive metadata is shown to the
          audience in synchronization with what they are hearing.</p>
        <p>The guest interface supports a single live audio source from a choice
          of any local microphone.</p>
        <p>The audience interface delivers the channel's broadcast mix, but also
          offers basic volume and <abbr title="equalizer">EQ</abbr> control
          plus the ability to pause/rewind/resume the live stream. Optionally,
          the listener can slow down the content of the audio without changing
          its pitch, for example to aid in understanding a foreign language.</p>
        <p>An advanced feature would give the audience control over the mix
          itself. The mix of tracks and sources created by the broadcaster would
          be a default, but the listener would have the ability to create a
          different mix. For instance, in the case of a radio play with a mix of
          voices, sound effects and music, the listener could be offered an
          interface to control the relative volume of the voices to effects and
          music, or create a binaural mix tailored specifically to their taste.
          Such a feature would provide valuable personalization of the radio
          experience, as well as significant accessibility enhancements.</p>
        <h3 id="notes-and-implementation-considerations-3">Notes and
          Implementation Considerations</h3>
        <ol>
          <li>
            <p>As with the Video Chat Application scenario, streaming and local
              device discovery and access within this scenario are handled by
              the <a href="http://www.w3.org/TR/webrtc/" title="WebRTC 1.0: Real-time Communication Between Browsers">Web
                Real-Time Communication API</a>. The local audio processing in
              this scenario highlights the requirement that <em>RTC streams and
                Web Audio be tightly integrated</em>. Incoming MediaStreams must
              be able to be exposed as audio sources, and audio destinations
              must be able to yield an outgoing RTC stream. For example, the
              broadcaster's browser employs a set of incoming MediaStreams from
              microphones, remote participants, etc., locally processes their
              audio through a graph of <code>AudioNodes</code>, and directs the
              output to an outgoing MediaStream representing the live mix for
              the show.</p>
          </li>
          <li>
            <p>Building this application requires the application of <em>gain
                control</em>, <em>panning</em>, <em>audio effects</em> and <em>blending</em>
              of multiple <em>mono and stereo audio sources</em> to yield a
              stereo mix. Some relevant features in the API include <code>AudioGainNode</code>,
              <code>ConvolverNode</code>, <code>AudioPannerNode</code>.</p>
          </li>
          <li>
            <p><em>Noise gating</em> (suppressing output when a source's level
              falls below some minimum threshold) is highly desirable for
              microphone inputs to avoid stray room noise being included in the
              broadcast mix. This could be implemented as a custom algorithm
              using a <code>JavaScriptAudioNode</code>.</p>
          </li>
          <li>
            <p>To drive the visual feedback to the broadcaster on audio source
              activity and to control automatic ducking, this scenario needs a
              way to easily <em>detect the time-averaged signal level</em> on a
              given audio source. The Web Audio API does not currently provide a
              prepackaged way to do this, but it can be implemented with custom
              JS processing or an ultra-low-pass filter built with <code>BiquadFilterNode</code>.</p>
          </li>
          <li>
            <p>Ducking affects the level of multiple audio sources at once,
              which implies the ability to associate a single <em>dynamic audio
                parameter</em> to the gain associated with these sources' signal
              paths. The specification's <code>AudioGain</code> interface
              provides this.</p>
          </li>
          <li>
            <p>Smooth muting requires the ability to <em>smoothly automate gain
                changes</em> over a time interval, without using
              browser-unfriendly coding techniques like tight loops or
              high-frequency callbacks. The <em>parameter automation</em>
              features associated with <code>AudioParam</code> are useful for
              this kind of feature.</p>
          </li>
          <li>
            <p>Pausing and resuming the show on the audience side implies the
              ability to <em>buffer data received from audio sources</em> in
              the processing graph, and also to <em>send buffered data to audio
                destinations</em>.</p>
          </li>
          <li>
            <p>Speed changes are currently unsupported by the Web Audio API.
              Thus, the functionality for audio speed changing, a custom
              algorithm, requires the ability to <em>create custom audio
                transformations</em> using a browser programming language (e.g.
              <code>JavaScriptAudioNode</code>). When audio delivery is slowed
              down, audio samples will have to be locally buffered by the
              application up to some allowed limit, since they continue to be
              delivered by the incoming stream at a normal rate.</p>
          </li>
          <li>
            <p>There is a standard way to access a set of <em>metadata
                properties for media resources</em> with the following <abbr title="World Wide Web Consortium">W3C</abbr>
              documents:</p>
            <ul>
              <li>
                <p> <a href="http://www.w3.org/TR/mediaont-10/" title="http://www.w3.org/TR/mediaont-10/">Ontology
                    for Media Resources 1.0</a>. This document defines a core
                  set of metadata properties for media resources, along with
                  their mappings to elements from a set of existing metadata
                  formats. </p>
              </li>
              <li>
                <p> <a href="http://www.w3.org/TR/mediaont-api-1.0/" title="http://www.w3.org/TR/mediaont-api-1.0/">API
                    for Media Resources 1.0</a>. This API provides developers
                  with a convenient access to metadata information stored in
                  different metadata formats. It provides means to access the
                  set of metadata properties defined in the Ontology for Media
                  Resources 1.0 specification. </p>
              </li>
            </ul>
          </li>
          <li>The ability for the listeners to create their own mix rely on the
            possibility of sending multiple tracks in the RTC stream. This is in
            scope of the current WebRTC specification, where one <code>MediaStream</code>
            can have multiple <code>MediaStreamTrack</code>s.</li>
        </ol>
      </section>
      <section id="music-creation-environment-with-sampled-instruments">
        <h3><span class="secno">2.5 </span>Music Creation Environment with
          Sampled Instruments</h3>
        <p>A composer is employing a web-based application to create and edit a
          musical composition with live synthesized playback. The user interface
          for composing can take a number of forms including conventional
          Western notation and a piano-roll style display. The document can be
          sonically rendered on demand as a piece of music, <em>i.e.</em> a
          series of precisely timed, pitched and modulated audio events (notes).
        </p>
        <p>The musician occasionally stops editing and wishes to hear playback
          of some or all of the score they are working on to take stock of their
          work. At this point the program performs sequenced playback of some
          portion of the document. Some simple effects such as instrument
          panning and room reverb are also applied for a more realistic and
          satisfying effect.</p>
        <p>Compositions in this editor employ a set of instrument samples, i.e.
          a pre-existing library of recorded audio snippets. Any given snippet
          is a brief audio recording of a note played on an instrument with some
          specific and known combination of pitch, dynamics and articulation.
          The combinations in the library are necessarily limited in number to
          avoid bandwidth and storage overhead. During playback, the editor must
          simulate the sound of each instrument playing its part in the
          composition. This is done by transforming the available pre-recorded
          samples from their original pitch, duration and volume to match the
          characteristics prescribed by each note in the composed music. These
          per-note transformations must also be scheduled to be played at the
          times prescribed by the composition.</p>
        <p>During playback a moving cursor indicates the exact point in the
          music that is being heard at each moment.</p>
        <p>At some point the user exports an MP3 or WAV file from the program
          for some other purpose. This file contains the same audio rendition of
          the score that is played interactively when the user requested it
          earlier.</p>
        <h3 id="notes-and-implementation-considerations-4">Notes and
          Implementation Considerations</h3>
        <ol>
          <li>
            <p> Instrument samples must be able to be loaded into memory for
              fast processing during music rendering. These pre-loaded audio
              snippets must have a one-to-many relationship with objects in the
              Web Audio API representing specific notes, to avoid duplicating
              the same sample in memory for each note in a composition that is
              rendered with it. The API's <code>AudioBuffer</code> and <code>AudioBufferSourceNode</code>
              interfaces address this requirement.</p>
          </li>
          <li>
            <p>It must be possible to schedule large numbers of individual
              events over a long period of time, each of which is a
              transformation of some original audio sample, without degrading
              real-time browser performance. A graph-based approach such as that
              in the Web Audio API makes the construction of any given
              transformation practical, by supporting simple recipes for
              creating sub-graphs built around a sample's pre-loaded <code>AudioBuffer</code>.
              These subgraphs can be constructed and scheduled to be played in
              the future. In one approach to supporting longer compositions, the
              construction and scheduling of future events can be kept "topped
              up" via periodic timer callbacks, to avoid the overhead of
              creating huge graphs all at once.</p>
          </li>
          <li>
            <p>A given sample must be able to be arbitrarily transformed in
              pitch and volume to match a note in the music. <code>AudioBufferSourceNode</code>'s
              <code>playbackRate</code> attribute provides the pitch-change
              capability, while <code>AudioGainNode</code> allows the volume to
              be adjusted.</p>
          </li>
          <li>
            <p>A given sample must be able to be arbitrarily transformed in
              duration (without changing its pitch) to match a note in the
              music. <code>AudioBufferSourceNode</code>'s looping parameters
              provide sample-accurate start and end loop points, allowing a note
              of arbitrary duration to be generated even though the original
              recording may be brief.</p>
          </li>
          <li>
            <p>Looped samples by definition do not have a clean ending. To avoid
              an abrupt glitchy cutoff at the end of a note, a gain and/or
              filter envelope must be applied. Such envelopes normally follow an
              exponential trajectory during key time intervals in the life cycle
              of a note. The <code>AudioParam</code> features of the Web Audio
              API in conjunction with <code>AudioGainNode</code> and <code>BiquadFilterNode</code>
              support this requirement.</p>
          </li>
          <li>
            <p> It is necessary to coordinate visual display with sequenced
              playback of the document, such as a moving cursor or highlighting
              effect applied to notes. This implies the need to programmatically
              determine the exact time offset within the performance of the
              sound being currently rendered through the computer's audio output
              channel. This time offset must, in turn, have a well-defined
              relationship to time offsets in prior API requests to schedule
              various notes at various times. The API provides such a capability
              in the <code>AudioContext.currentTime</code> attribute.</p>
          </li>
          <li>
            <p> To export an audio file, the audio rendering pipeline must be
              able to yield buffers of sample frames directly, rather than being
              forced to an audio device destination. Built-in codecs to
              translate these buffers to standard audio file output formats are
              also desirable.</p>
          </li>
          <li>
            <p>Typical per-channel effects such as stereo pan control must be
              readily available. Panning allows the sound output for each
              instrument channel to appear to occupy a different spatial
              location in the output mix, adding greatly to the realism of the
              playback. Adding and configuring one of the Web Audio API's <code>AudioPannerNode</code>
              for each channel output path provides this capability.</p>
          </li>
          <li>
            <p>Typical master bus effects such as room reverb must be readily
              available. Such effects are applied to the entire mix as a final
              processing stage. A single <code>ConvolverNode</code> is capable
              of simulating a wide range of room acoustics.</p>
          </li>
        </ol>
      </section>
      <section id="connected-dj-booth">
        <h3><span class="secno">2.6 </span>Connected <abbr title="Disc Jockey">DJ</abbr>
          booth</h3>
        <p>A popular <abbr title="Disc Jockey">DJ</abbr> is playing a live set,
          using a popular web-based <abbr title="Disc Jockey">DJ</abbr>
          software. The web application allows her to perform both in the club
          where she is mixing, as well as online, with tens of thousands joining
          live to enjoy the set.</p>
        <p>The <abbr title="Disc Jockey">DJ</abbr>-deck web interface offers
          the typical features of decks and turntables. While a first track is
          playing and its sound sent to both the sound system in the club and
          streamed to the web browsers of fans around the world, the <abbr title="Disc Jockey">DJ</abbr>
          would be able to quickly select several other track, play them through
          headphones without affecting the main audio output of the application,
          and match them to the track currently playing through a mix of
          pausing, skipping forward or back and pitch/speed change. The
          application helps automate a lot of this work: by measuring the beat
          of the current track at 125BPM and the one of the chosen next track at
          140 <abbr title="Beats Per Minute">BPM</abbr>, it can automatically
          slow down the second track, and even position it to match the beats of
          the one currently playing. </p>
        <p>Once the correct match is reached, The <abbr title="Disc Jockey">DJ</abbr>
          would be able to start playing the track in the main audio output,
          either immediately or by slowly changing the volume controls for each
          track. She uses a cross fader to let the new song blend into the old
          one, and eventually goes completely across so only the new song is
          playing. This gives the illusion that the song never ended.</p>
        <p>At the other end, fans listening to the set would be able to watch a
          video of the <abbr title="Disc Jockey">DJ</abbr> mixing, accompanied
          by a graphic visualization of the music, picked from a variety of
          choices: spectrum analysis, level-meter view or a number of 2D or 3D
          abstract visualizations displayed either next to or overlaid on the <abbr

            title="Disc Jockey">DJ</abbr> video.</p>
        <h3 id="notes-and-implementation-considerations-5">Notes and
          Implementation Considerations</h3>
        <ol>
          <li>As in many other scenarios in this document, it is expected that
            APIs such as the <a href="http://www.w3.org/TR/webrtc/" title="WebRTC 1.0: Real-time Communication Between Browsers">Web
              Real-Time Communication API</a> will be used for the streaming of
            audio and video across a number of clients.</li>
          <li>
            <p>One of the specific requirements illustrated by this scenario is
              the ability to have two different outputs for the sound: one for
              the headphones, and one for the music stream sent to all the
              clients. With the typical web-friendly hardware, this would be
              difficult or impossible to implement by considering both as audio
              destinations, since they seldom have or allow two sound outputs to
              be used at the same time. And indeed, in the current Web Audio API
              draft, a given <code>AudioContext</code> can only use one <code>AudioDestinationNode</code>
              as destination.</p>
            <p>However, if we consider that the headphones are the audio output,
              and that the streaming <abbr title="Disc Jockey">DJ</abbr> set is
              not a typical audio destination but an outgoing <code>MediaStream</code>
              passed on to the WebRTC API, it should be possible to implement
              this scenario, sending output to both headphones and the stream
              and gradually sending sound from one to the other without
              affecting the exact state of playback and processing of a source.
              With the Web Audio API, this can be achieved by using the <code>createMediaStreamDestination()</code>
              interface.</p>
          </li>
          <li>This scenario makes heavy usage of audio analysis capabilities,
            both for automation purposes (beat detection and beat matching) and
            visualization (spectrum, level and other abstract visualization
            modes).</li>
          <li>The requirement for pitch/speed change are not currently covered
            by the Web Audio API's native processing nodes. Such processing
            would probably have to be handled with custom processing nodes.</li>
        </ol>
      </section>
      <section id="playful-sonification-of-user-interfaces">
        <h3><span class="secno">2.7 </span>Playful sonification of user
          interfaces</h3>
        <p>A child is visiting a social website designed for kids. The playful,
          colorful HTML interface is accompanied by sound effects played as the
          child hovers or clicks on some of the elements of the page. For
          example, when filling in a form the sound of a typewriter can be heard
          as the child types in the form field. Some of the sounds are
          spatialized and have a different volume depending on where and how the
          child interacts with the page. When an action triggers a download
          visualized with a progress bar, a gradually rising pitch sound
          accompanies the download and another sound (ping!) is played when the
          download is complete.</p>
        <h3 id="notes-and-implementation-considerations-6">Notes and
          Implementation Considerations</h3>
        <ol>
          <li>
            <p>Although the web <abbr title="User Interface">UI</abbr>
              incorporates many sound effects, its controls are embedded in the
              site's pages using standard web technology such as HTML form
              elements and CSS stylesheets. JavaScript event handlers may be
              attached to these elements, causing graphs of <code>AudioNodes</code>
              to be constructed and activated to produce sound output.</p>
          </li>
          <li>
            <p>Modularity, spatialization and mixing play an important role in
              this scenario, as for the others in this document.</p>
          </li>
          <li>
            <p>Various effects can be achieved through programmatic variation of
              these sounds using the Web Audio API. The download progress could
              smoothly vary the pitch of an <code>AudioBufferSourceNode</code>'s
              <code>playbackRate</code> using an exponential ramp function, or a
              more realistic typewriter sound could be achieved by varying an
              output filter's frequency based on the keypress's character code.</p>
          </li>
          <li>
            <p>In a future version of CSS, stylesheets may be able to support
              simple types of sonification, such as attaching a "typewriter key"
              sound to an HTML <code>textarea</code> element or a "click" sound
              to an HTML <code>button</code>. These can be thought of as an
              extension of the visual skinning concepts already embodied by
              style attributes such as <code>background-image</code>.</p>
          </li>
        </ol>
      </section>
      <section id="podcast-on-a-flight">
        <h3><span class="secno">2.8 </span>Podcast on a flight</h3>
        <p>A traveler is subscribed to a podcast, and has previously downloaded
          an audio book on his device using the podcast's web-based application.
          The audio files are stored locally on his device, giving simple and
          convenient access to episodic content whenever the user wishes to
          listen.</p>
        <p>Sitting in an airplane for a 2-hour flight, he opens the podcast
          application in his HTML browser and sees that the episode he has
          selected lasts 3 hours. The application offers a speed-up feature that
          allows the speech to be delivered at a faster than normal speed
          without pitch distortion ("chipmunk voices"). He sets the audition
          time to 2 hours in order to finish the audio book before landing. He
          also sets the sound control in the application to "Noisy Environment",
          causing the sound to be equalized for greatest intelligibility in a
          noisy setting such as an airplane.</p>
        <h3 id="notes-and-implementation-considerations-7">Notes and
          Implementation Considerations</h3>
        <ol>
          <li>
            <p>Local audio can be downloaded, stored and retrieved using the <a

                href="http://www.w3.org/TR/FileAPI/">HTML File API</a>.</p>
          </li>
          <li>
            <p>This scenario requires a special audio transformation that can
              compress the duration of speech without affecting overall timbre
              and intelligibility. In the Web Audio API this function isn't
              natively supported but could be accomplished through attaching
              custom processing code to a <code>JavaScriptAudioNode</code>.</p>
          </li>
          <li>
            <p>The "Noisy Environment" setting could be accomplished through
              equalization features in the Web Audio API such as <code>BiquadFilterNode</code>
              or <code>ConvolverNode</code>.</p>
          </li>
        </ol>
      </section>
      <section id="short-film-with-director-s-commentary-and-audio-description">
        <h3><span class="secno">2.9 </span>Short film with director's
          commentary and audio description</h3>
        <p>A video editor is using an online editing tool to refine the
          soundtrack of a short film. Once the video is ready, she will work
          with the production team to prepare an audio description of the scenes
          to make the video work more accessible to people with sight
          impairments. The video director is also planning to add an audio
          commentary track to explain the creative process behind the film.</p>
        <p>Using the online tool, the video editor extracts the existing
          recorded vocals from the video stream, modifies their levels and
          performs other modifications of the audio stream. She also adds
          several songs, including a orchestral background and pop songs, at
          different parts of the film soundtrack. Several Foley effects
          (footsteps, doors opening and closing, etc.) are also added to make
          the soundscape of each scene complete. </p>
        <p>While editing, the online tool must ensure that the audio and video
          playback are synchronized, allowing the editor to insert audio samples
          at the right time. As the length of one of the songs is slightly
          different from the video segment she is matching it with, she can
          synchronize the two by slightly speeding up or slowing down the audio
          track. The final soundtrack is mixed down into the final soundtrack,
          added to the video as a replacement for the original audio track, and
          synced with the video track.</p>
        <p>Once the audio description and commentary are recorded, the film,
          displayed in a HTML web page, can be played with its original audio
          track (embedded in the video container) or with any of the audio
          commentary tracks loaded from a different source and synchronized with
          the video playback. When there's audio on the commentary track, the
          main track volume is reduced (ducked) gradually and smoothly brought
          back to full volume when the commentary / description track is silent.
          The visitor can switch between audio tracks on the fly, without
          affecting the video playback. Pausing the video playback also pauses
          the commentary track, which then remains in sync when playback
          resumes.</p>
        <h3 id="notes-and-implementation-considerations-8">Notes and
          Implementation Considerations</h3>
        <ol>
          <li>This scenario is, in many ways, fairly similar to a number of
            others already discussed throughout the document. The ability to lay
            out a number of sources and mix them in a consistent soundtrack is
            the subject of the <a href="#online-music-production-tool">Online
              music production tool</a> scenario, while some effects such as
            ducking have already been discussed in the <a href="#online-radio-broadcast">Online
              radio broadcast</a> scenario.</li>
          <li>Essentially, this use case illustrates the need to do all these
            things in sync with video. In the context of the open web platform,
            it means that audio processing API to integrate with the <a href="http://www.w3.org/TR/html5/embedded-content-0.html#media-controllers">HTML5
              <code>MediaController</code> interface</a>.</li>
        </ol>
      </section>
      <section id="web-based-guitar-practice-service">
        <h3><span class="secno">2.10 </span>Web-based guitar practice service </h3>
        <p>A serious guitar player uses a web-based tool to practice a new tune.
          Connecting a USB microphone and a pair of headphones to their
          computer, the guitarist is able to tune an acoustic guitar using a
          graphical interface and set a metronome for the practice session. A
          mix of one or more backing tracks can be optionally selected for the
          guitarist to play along with, with or without the metronome present.</p>
        <p>During a practice session, the microphone audio is analyzed to
          determine whether the guitarist is playing the correct notes in tempo,
          and visual feedback is provided via a graphical interface of guitar
          tablature sheet music with superimposed highlighting.</p>
        <p>The guitarist's performance during each session is recorded,
          optionally mixed with the audio backing-track mix. At the conclusion
          of a session, this performance can be saved to various file formats or
          uploaded to an online social music service for sharing and commentary
          with other users.</p>
        <h3 id="notes-and-implementation-considerations-9">Notes and
          Implementation Considerations</h3>
        <ol>
          <li>
            <p>The audio input reflects the guitarist's performance, which is
              itself aurally synchronized by the guitarist to the current audio
              output. The scenario requires that the input be analyzed for
              correct rhythmic and pitch content. Such an algorithm can be
              implemented in a <code>JavaScriptAudioNode</code>.</p>
          </li>
          <li>
            <p>Analysis of the performance in turn requires measurement of the
              real-time latency in both audio input and output, so that the
              algorithm analyzing the live performance can know the temporal
              relationship of a given output sample (reflecting the metronome
              and/or backing track) to a given input sample (reflecting the
              guitarist playing along with that output). These latencies are
              unpredictable from one system to another and cannot be hard-coded.
              Currently the Web Audio API lacks such support.</p>
          </li>
          <li>
            <p>This scenario uses a mixture of sound sources including a live
              microphone input, a synthesized metronome and a set of
              pre-recorded audio backing tracks (which are synced to a fixed
              tempo). The mixing of these sources to the browser's audio output
              can be accomplished by a combination of instances of <code>AudioGainNode</code>
              and <code>AudioPannerNode</code>.</p>
          </li>
          <li>
            <p>The live input requires microphone access, which it is
              anticipated will be available via <a href="http://www.w3.org/TR/html-media-capture/"

                title="HTML Media Capture">HTML Media Capture</a> bridged
              through an AudioNode interface.</p>
          </li>
          <li>
            <p>Pre-recorded backing tracks can be loaded into <code>AudioBuffers</code>
              and used as sample-accurate synced sources by wrapping these in <code>AudioBufferSourceNode</code>
              instances.</p>
          </li>
          <li>
            <p>Metronome synthesis can be accomplished with a variety of means
              provided by the Web Audio API. In one approach, an implementer
              could use an <code>Oscillator</code> square-wave source to
              generate the metronome sound. A timer callback repeatedly runs at
              a low frequency to maintain a pool of these instances scheduled to
              occur on future beats in the music (which can be sample-accurately
              synced to offsets in the backing tracks given the lock-step timing
              in the Web Audio API).</p>
          </li>
          <li>
            <p>Programmatic output of a recorded session's audio buffer must be
              accomplished to files (via the HTML5 File API) or upload streams
              (via MediaStreams or HTTP). The scenario implies the use of one or
              more encoders on this buffered data to yield the supported audio
              file formats. Native audio-to-file encoding is not currently
              supported by the Web Audio API and thus would need to be
              implemented in JavaScript.</p>
          </li>
        </ol>
      </section>
      <section id="user-control-of-audio">
        <h3><span class="secno">2.11 </span>User Control of Audio</h3>
        <p>A programmer wants to create a browser extension to allow the user to
          control the volume of audio.</p>
        <p>The extension should let the user control the audio volume on a
          per-tab basis, or to kill any audio playing completely. The extension
          developer wishes to make sure killing the audio is done in a way that
          takes care of garbage collection.</p>
        <p>Among the features sometimes requested for his extension are the
          ability to limit the audio volume to an acceptable level, both per tab
          and globally. On operating systems that allow it, the developer would
          also like his extension to mute or pause sound when a critical system
          sound is being played.</p>
        <h3 id="notes-and-implementation-considerations-10">Notes and
          Implementation Considerations</h3>
        <ol>
          <li>
            <p>This function is likely to combine usage of both a
              browser-specific extension API and the Web Audio API. One way to
              implement this scenario would be to use a browser-dependent API to
              iterate through a list of window objects, and then for each window
              object iterate through a list of active AudioContexts and manage
              their volume (or, more conveniently, manage some kind of master
              audio volume for the window). Neither of these latter approaches
              are currently supported by the Web Audio API.</p>
          </li>
          <li>
            <p>The ability to mute or pause sounds when the Operating System
              fires a critical system sound is modelled after the feature in
              existing Operating Systems which will automatically mute
              applications when outputting a system sound. As such, this may not
              involve any specific requirement for the Web Audio API. However,
              because some operating systems may implement such a feature, Web
              Audio apps may want to be notified of the muting and act
              accordingly (suspend, pause, etc). There may therefore be a
              requirement for the Web Audio API to provide such an event
              handler.</p>
          </li>
        </ol>
      </section>
    </section>
    <section id="acknowledgements" class="appendix">
      <!--OddPage-->
      <h2><span class="secno">A. </span>Acknowledgements</h2>
      <p>This document is the result of the work of the <abbr title="World Wide Web Consortium">W3C</abbr>
        <a href="http://www.w3.org/2011/audio/">Audio Working Group</a>. Members
        of the working group, at the time of publication, included: </p>
      <ul>
        <li>Bateman, Adrian (Microsoft Corporation);</li>
        <li>Berkovitz, Joe (Invited expert);</li>
        <li>Cardoso, Gabriel (INRIA);</li>
        <li>Carlson, Eric (Apple, Inc.);</li>
        <li>Chen, Bin (Baidu, Inc.);</li>
        <li>Geelnard, Marcus (Opera Software);</li>
        <li>Goode, Adam (Google, Inc.);</li>
        <li>Gregan, Matthew (Mozilla Foundation);</li>
        <li>JÃ¤genstedt, Philip (Opera Software);</li>
        <li>Kalliokoski, Jussi (Invited expert);</li>
        <li>Lowis, Chris (British Broadcasting Corporation);</li>
        <li>MacDonald, Alistair (Invited Expert);</li>
        <li>Mandyam, Giridhar (Qualcomm Innovation Center, Inc);</li>
        <li>Michel, Thierry (<abbr title="World Wide Web Consortium">W3C</abbr>/<abbr

            title="European Research Consortium for Informatics and Mathematics">ERCIM</abbr>);</li>
        <li>Noble, Jer (Apple, Inc.);</li>
        <li>O'Callahan, Robert (Mozilla Foundation);</li>
        <li>Olivier, Frank (Microsoft Corporation);</li>
        <li>Paradis, Matthew (British Broadcasting Corporation);</li>
        <li>Peraza Barreras, Jorge Armando (Microsoft Corporation);</li>
        <li>Raman, T.V. (Google, Inc.);</li>
        <li>Rogers, Chris (Google, Inc.);</li>
        <li>Schepers, Doug (<abbr title="World Wide Web Consortium">W3C</abbr>/<abbr

            title="Massachusetts Institute of Technology">MIT</abbr>);</li>
        <li>Shires, Glen (Google, Inc.);</li>
        <li>Smith, Michael (<abbr title="World Wide Web Consortium">W3C</abbr>/Keio);</li>
        <li>Thereaux, Olivier (British Broadcasting Corporation);</li>
        <li>Wei, James (Intel Corporation);</li>
        <li>Wilson, Chris (Google,Inc.); </li>
        <li>Young, Milan (Nuance Communications, Inc.).</li>
      </ul>
      <p>The people who have contributed to <a href="http://lists.w3.org/Archives/Public/public-audio/">discussions
          on public-audio@w3.org</a> are also gratefully acknowledged. </p>
      <p>This document was also heavily influenced by earlier work by the audio
        working group and others, including:</p>
      <ul>
        <li>A list of "<a href="http://www.w3.org/2005/Incubator/audio/wiki/Audio_API_Use_Cases"

            title="Audio API Use Cases - Audio Incubator">Core Use Cases</a>"
          authored by the <a href="http://www.w3.org/2005/Incubator/audio/" title="W3C Audio Incubator Group"><abbr

              title="World Wide Web Consortium">W3C</abbr> Audio Incubator Group</a>,
          which predated the <abbr title="World Wide Web Consortium">W3C</abbr>
          Audio Working Group</li>
        <li> The <a href="http://tools.ietf.org/html/draft-ietf-rtcweb-use-cases-and-requirements-06#section-4.2"

            title="http://tools.ietf.org/html/draft-ietf-rtcweb-use-cases-and-requirements-06#section-4.2">use
            cases requirements from Web RTC</a></li>
        <li> The <a href="http://www.w3.org/TR/2011/WD-streamproc-20111215/#scenarios"

            title="http://www.w3.org/TR/2011/WD-streamproc-20111215/#scenarios">Scenarios
            from the Media Streams Processing</a></li>
      </ul>
    </section>
    
  </body>
</html>
